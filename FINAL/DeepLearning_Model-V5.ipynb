{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@Project: Connexin Group \\n\\n@FileName: DeepLearning_Model_V3\\nf\\n@Author：Zhejian Peng\\n\\n@Create date: Mar. 25th, 2018\\n\\n@description：reduce dimension of our dataset using pca, without normalizing categorical data.\\n\\n@Update date：Mar. 25th, 2018\\n            Try to split train and test before normalization.\\n            1. Need to update normalization for zipcode on V3\\n            2. Update drop2 to drop more features that might leak information\\n            \\n            Update_deep learing model-V and make comparison with logistic Regression\\n            \\n            April. 6th, 2018 V3\\n            1. Deploy Model for validation\\n            2. Visualize our Deep Learning Model\\n            \\n            April. 21, 2018 V5\\n            1. Use finalized datasets.\\n            2. Try Overfit the model and finalize the model\\n@Vindicator：  \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@Project: Connexin Group \n",
    "\n",
    "@FileName: DeepLearning_Model_V3\n",
    "f\n",
    "@Author：Zhejian Peng\n",
    "\n",
    "@Create date: Mar. 25th, 2018\n",
    "\n",
    "@description：reduce dimension of our dataset using pca, without normalizing categorical data.\n",
    "\n",
    "@Update date：Mar. 25th, 2018\n",
    "            Try to split train and test before normalization.\n",
    "            1. Need to update normalization for zipcode on V3\n",
    "            2. Update drop2 to drop more features that might leak information\n",
    "            \n",
    "            Update_deep learing model-V and make comparison with logistic Regression\n",
    "            \n",
    "            April. 6th, 2018 V3\n",
    "            1. Deploy Model for validation\n",
    "            2. Visualize our Deep Learning Model\n",
    "            \n",
    "            April. 21, 2018 V5\n",
    "            1. Use finalized datasets.\n",
    "            2. Try Overfit the model and finalize the model\n",
    "@Vindicator：  \n",
    "\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Select all categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in our data Frame\n",
    "def readcsv(file_path):\n",
    "    LARGE_FILE = file_path\n",
    "    CHUNKSIZE = 100000 # processing 100,000 rows at a time\n",
    "    # Add encoding encoding = \"ISO-8859-1\", why?\n",
    "    reader = pd.read_csv(LARGE_FILE, chunksize=CHUNKSIZE, low_memory=False, encoding = \"ISO-8859-1\")\n",
    "    frames = []\n",
    "    for df in reader:\n",
    "        frames.append(df)\n",
    "    loan_data = pd.concat(frames)\n",
    "    return loan_data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"/Users/zhejianpeng/Google Drive File Stream/My Drive/MSFE-UIUC/MSFE-TWO/Practicum/Week7/loan_data_no_current_converted.csv\"\n",
    "loan_data = readcsv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = loan_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert Verification_status_joint, add this categorical data to the categorical list\n",
    "for idx, i in df[\"verification_status_joint\"].iteritems():\n",
    "    if i == \"Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 1\n",
    "    elif i == \"Source Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 2\n",
    "    elif i == \"Not Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical = ['grade', 'sub_grade', 'emp_length', 'purpose', 'title', 'application_type', 'hardship_flag', 'hardship_type', 'hardship_reason', \n",
    "              'hardship_status', 'hardship_loan_status', 'settlement_status', 'disbursement_method', 'home_ownership',\n",
    "              'pymnt_plan', 'debt_settlement_flag', 'title', 'initial_list_status', 'loan_status', 'verification_status',\n",
    "              'term', 'verification_status_joint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 categorical data in our dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d categorical data in our dataset.\" % len(categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in engineered features.\n",
    "FE_PATH = \"./This_week_FE.csv\"\n",
    "df_FE = readcsv(FE_PATH)\n",
    "# temp.replace(float('nan'), -9999999, inplace =True)\n",
    "# temp = df_FE[df_FE['ratio_rev_acct']!='#DIV/0!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FE['ratio_rev_acct'].replace('#DIV/0!', float(-np.inf), inplace = True)\n",
    "df_FE.loc[:,'ratio_rev_acct'] = [float(x) for x in df_FE['ratio_rev_acct']]\n",
    "\n",
    "df_FE['ratio_rev_acct'].replace(float(-np.inf), np.max(df_FE['ratio_rev_acct']), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_FE['loan_amt_to_avg_inc'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season\n",
      "loan_amt_to_avg_inc\n",
      "coll_to_cur\n"
     ]
    }
   ],
   "source": [
    "# Replace '#DIV/0!' with the max of each col\n",
    "for i in df_FE.columns[0:3]:\n",
    "    df_FE[i].replace('#DIV/0!', np.max(df_FE[i]), inplace = True)\n",
    "    df_FE.loc[:,i] = [float(x) for x in df_FE[i]]\n",
    "\n",
    "    print(i)\n",
    "\n",
    "df = pd.concat([df,df_FE],axis=1)\n",
    "assert(df_FE.shape[0] == df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我写了一堆code 然后发现其实简单一点就能弄出来，所以大家忽略后面的code！！！\n",
    "# I have wrote a lot of code for this only to find out that I only need this simple function!!!\n",
    "def norm_inc_by_zip(zipcode, income):\n",
    "    '''\n",
    "        @description: Use on a column of data; output a dictionary that returns mean and average in each zipcode area\n",
    "        @zipcode： zipcode dataframe column\n",
    "        @income: income df column \n",
    "        @return:      return a dictionary\n",
    "    '''  \n",
    "    # I try to replace nan with 0 for income, and nan in zipcode for \"000xx\"\n",
    "    df[\"annual_inc\"].fillna(0)\n",
    "    df[\"zip_code\"].fillna(\"000xx\")\n",
    "    \n",
    "    mean_var = {}\n",
    "    for idx, value in zipcode.iteritems():\n",
    "        # calculate total income\n",
    "        if value in mean_var:\n",
    "            mean_var[value].append(income[idx])\n",
    "        else:\n",
    "            mean_var[value] = [income[idx]]\n",
    "\n",
    "    \n",
    "    #assert(len(zip_code) == len(mean_var))\n",
    "    # compute the average income in each zip_code area\n",
    "    for key, value in mean_var.items():\n",
    "        # if there only one element, we set their variance to 1. This way when normalize, it will have a 0 z-score.\n",
    "        if len(value) == 1:\n",
    "            #print(value[0])\n",
    "            mean_var[key] = [value[0], 1]\n",
    "        else:\n",
    "            mean_var[key] = [np.mean(value), np.std(value)]\n",
    "        \n",
    "    # first loop through every annual income by calculate its z score. (Income - mean_by_zipcode) / variance_by_zipcode\n",
    "    for idx, value in df[\"zip_code\"].iteritems():\n",
    "        #inc_colnum = df.columns.get_loc(\"annual_inc\")\n",
    "        col_num_inc = df[\"annual_inc\"]\n",
    "        mean, std = mean_var[value]\n",
    "        df.at[idx, \"annual_inc\"] = (df.at[idx, \"annual_inc\"] - mean) / std\n",
    "    print(\"Income is successfually normalized\")\n",
    "    return mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income is successfually normalized\n"
     ]
    }
   ],
   "source": [
    "dic = norm_inc_by_zip(df[\"zip_code\"], df[\"annual_inc\"])\n",
    "df.drop('zip_code', axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Set, X, Y, Train/Test Sets And normalize it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "def percentage(x):\n",
    "    x = np.str(x)\n",
    "    if x[-1] == '%':\n",
    "        x = x[0:len(x)-1]\n",
    "    else:\n",
    "        print(x)\n",
    "    return float(x) / 100\n",
    "\n",
    "\n",
    "df['revol_util'] = [percentage(x) for x in df['revol_util']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dataset size is :  (851641, 58)\n"
     ]
    }
   ],
   "source": [
    "# drop the observation that was missing for any field\n",
    "df.dropna(axis=0, how='all', inplace=True)\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "# Use Finalized droplist provide by Yufei Gao\n",
    "drop_list=['installment','term','settlement_date','pymnt_plan','hardship_length', 'settlement_percentage', 'settlement_term', 'sec_app_earliest_cr_line','policy_code','hardship_end_date','settlement_amount',\n",
    "           'payment_plan_start_date','hardship_start_date','out_prncp','emp_title','title','earliest_cr_line','desc','issue_d','id','member_id','url','grade','sub_grade',\n",
    "                   'int_rate','avg_cur_bal','out_prncp_inv','debt_settlement_flag_date','hardship_amount','hardship_reason','addr_state','funded_amnt','funded_amnt_inv','collection_recovery_fee',\n",
    "                   'collections_12_mths_ex_med','mths_since_last_major_derog','next_pymnt_d','recoveries','total_pymnt',\n",
    "                   'total_pymnt_inv','total_rec_int','last_pymnt_d','last_credit_pull_d',\n",
    "                  'total_rec_prncp','settlement_status','hardship_loan_status','hardship_status','debt_settlement_flag',\n",
    "                   'verification_status','total_rec_late_fee','verification_status_joint','hardship_flag', 'hardship_type', 'hardship_reason'\n",
    "                    'hardship_status','hardship_loan_status','acc_now_delinq','delinq_amnt','deferral_term','hardship_amount'\n",
    "                    'hardship_length','hardship_dpd','hardship_payoff_balance_amount','hardship_last_payment_amount']\n",
    "\n",
    "\n",
    "# Drop drop_updated\n",
    "df.drop(drop_list, inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "# Drop all colums where value missed more than 20%\n",
    "num_rows=df.count(axis=0)\n",
    "df=df.iloc[:,(num_rows>=0.8*len(df)).tolist()]\n",
    "\n",
    "# Then fill rest of missing value with mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "# Drop all rows with 4,5,6\n",
    "'''for idx, i in loan_data[\"loan_status\"].iteritems():\n",
    "    if i == \"Fully Paid\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 1\n",
    "    elif i == \"Does not meet the credit policy. Status:Fully Paid\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 2\n",
    "    elif i == \"Does not meet the credit policy. Status:Charged Off\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 3\n",
    "    elif i == \"In Grace Period\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 4\n",
    "    elif i == \"Late (16-30 days)\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 5\n",
    "    elif i == \"Late (31-120 days)\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 6\n",
    "    elif i == \"Default\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 7\n",
    "    elif i == \"Charged Off\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 8'''\n",
    "\n",
    "\n",
    "# Let's test our result previous result, Previous result set Y to binary number 1,2.\n",
    "# 4,5,6 are dropped, and we dont care about ‘Credit policy'\n",
    "df = df[(df['loan_status']!=4) & (df['loan_status']!=5) & (df['loan_status']!=6)]\n",
    "print(\"Input Dataset size is : \",df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['revol_util']:\n",
    "    if math.isnan(i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select features that is not in categorical data to normalize: categorial[], verification_status_joint, and annual_inc\n",
    "# There categorical features are features need to include in X\n",
    "features= list(df.columns)\n",
    "features_need_norm = []\n",
    "categorical_features = []\n",
    "for i in features:\n",
    "    if i not in categorical and i != \"verification_status_joint\":\n",
    "        features_need_norm.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51 numerical features need normalization\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d numerical features need normalization\" %len(features_need_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = df.loc[:,['loan_status']].values\n",
    "features.remove(\"loan_status\")\n",
    "categorical_features.remove('loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can not have loan_status in X, we want to check this, if it prints \"Warning\", we have a problem!!!\n",
    "for i in categorical_features:\n",
    "    if i == \"loan_status\":\n",
    "        print(\"Warning!\")\n",
    "for i in features_need_norm:\n",
    "    if i == \"loan_status\":\n",
    "        print(\"Warning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our result previous result, Previous result set Y to binary number 1,2.\n",
    "\n",
    "Y = list(Y.reshape(len(Y)))\n",
    "for i in range(len(Y)):\n",
    "    if Y[i]==7 or Y[i]==8 or Y[i] == 3:\n",
    "        Y[i] = 1 # Default\n",
    "    else:\n",
    "        Y[i] = 0 # Fully Paid\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in Y:\n",
    "    if i != 0 and i!=1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I want to find the starting index and ending index of categorical data in X. \n",
    "# starting_col_index_of_categorical_data: starting index of categorical data in X\n",
    "# last_col_index_in_X: ending index of categorical data in X\n",
    "X = df.loc[:,features_need_norm].values\n",
    "starting_col_index_of_categorical_data = X.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.loc[:,categorical_features].values.shape = (891823, 16)\n",
    "X = np.concatenate((X, df.loc[:,categorical_features].values), axis=1)\n",
    "last_col_index_in_X = X.shape[1]-1\n",
    "# X.shape = (891823, 109)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical featurs in dataset: 6\n",
      "Number of numerical features in dataset: 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of categorical featurs in dataset:\", len(categorical_features))\n",
    "print(\"Number of numerical features in dataset:\", len(features_need_norm))\n",
    "# From column index 93 to column index 107 are 15 categorical data in X\n",
    "# len(categorical_features) = 15\n",
    "last_col_index_in_X-starting_col_index_of_categorical_data+1 == len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split train test set \n",
    "Y = np.ravel(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=1, test_size=0.25)\n",
    "# x_train, x_test, y_train, y_test = log_reg.split(X,Y,rand=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Normalize Train set and set mean variance in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize(numerical_features):\n",
    "    '''\n",
    "        @description: normalize all numerical features in X, and return a dictionary with mean variance of each features\n",
    "        @numerical_features： a numpy array of numerical features of shape (668867, 79)\n",
    "        @return: a dictionary contain mean variance of each features\n",
    "    '''  \n",
    "        # Step1: calculate mean variance of each columns in numerical features\n",
    "    epsilon = 10**-8\n",
    "    dic = {}\n",
    "    counter = 0\n",
    "    for columns in numerical_features.T:\n",
    "        mean  = np.mean(columns)\n",
    "        std = np.std(columns)\n",
    "        dic[counter] = [mean, std]\n",
    "        counter +=1\n",
    "\n",
    "    assert(counter == numerical_features.shape[1])\n",
    "\n",
    "    # Step2: Normalize numerical_features\n",
    "    for key, val in dic.items():\n",
    "        try :\n",
    "        #numerical_features[:,key] = (numerical_features[:,key] - val[0]) / val[1]\n",
    "            numerical_features[:, key] = (numerical_features[:,key] - val[0]) / val[1]\n",
    "            assert(np.mean(numerical_features[:, key])- 0.0 < epsilon)\n",
    "            assert(np.std(numerical_features[:, key])- 1.0 < epsilon)\n",
    "        except AssertionError:\n",
    "            print(np.mean(numerical_features[:, key]),\" | \", np.std(numerical_features[:, key]))\n",
    "            print(numerical_features[:,key], \"Key: \", key)\n",
    "            print(\"SUM: \", np.sum(numerical_features[:, key]))\n",
    "    return dic\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [3.5, 2.5], 1: [4.5, 2.5], 2: [5.5, 2.5], 3: [6.5, 2.5], 4: [7.5, 2.5]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "normalize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mv_dic = normalize(x_train[:,0: starting_col_index_of_categorical_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize test set according to train set's mean variance\n",
    "for key,val in mv_dic.items():\n",
    "    x_test[:, key] = (x_test[:,key] - val[0] ) / val[1]\n",
    "    # print(np.mean(x_test[:, key]))\n",
    "    # print(np.std(x_test[:, key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ratio \n",
      " 0.20687047639624068  |  0.7931295236037593\n",
      "Train set ratio \n",
      " 0.2065348425782412  |  0.7934651574217588\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in y_test:\n",
    "    if i==1:\n",
    "        counter +=1\n",
    "    \n",
    "print('Train set ratio \\n', counter/len(y_test), \" | \", (len(y_test)-counter) / len(y_test))\n",
    "    \n",
    "counter = 0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        counter +=1\n",
    "    \n",
    "print('Train set ratio \\n', counter/len(y_train), \" | \", (len(y_train)-counter) / len(y_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.791047\n",
       "8    0.205585\n",
       "2    0.002334\n",
       "3    0.000894\n",
       "7    0.000140\n",
       "Name: loan_status, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loan_status\"].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Train a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with ADAM and Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train:  (63873, 57)\n",
      "shape of X_test:  (212911, 57)\n",
      "shape of Y_train: (63873,)\n",
      "shape of Y_test: (212911,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "# Use small data set to test for overfitting\n",
    "np.random.seed(1)\n",
    "rand_numbers = np.random.randint(0, len(x_train), int(len(x_train)*0.1))\n",
    "X_train = x_train[rand_numbers]\n",
    "X_test = x_test\n",
    "Y_train = y_train[rand_numbers]\n",
    "Y_test = y_test\n",
    "\n",
    "# X_train = x_train\n",
    "# X_test = x_test\n",
    "\n",
    "# Y_train = y_train\n",
    "# Y_test = y_test\n",
    "\n",
    "print(\"shape of X_train: \", X_train.shape)\n",
    "print(\"shape of X_test: \", X_test.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Keras to construct a sequential model and visualize it \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "import keras\n",
    "import keras.backend as tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_dnn.reset_states()\n",
    "model_dnn = Sequential()\n",
    "dim = X_train.shape[1]\n",
    "model_dnn.add(Dense(64, activation='relu', input_dim=dim, kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "#model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(32, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\n",
    "model_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "# model.add(Flatten())\n",
    "model_dnn.add(Dense(2, activation='softmax'))\n",
    "# adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)\n",
    "# model.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "63873/63873 [==============================] - 8s 133us/step - loss: 0.3222 - acc: 0.8394\n",
      "Epoch 2/2000\n",
      "63873/63873 [==============================] - 6s 88us/step - loss: 0.2674 - acc: 0.8624\n",
      "Epoch 3/2000\n",
      "63873/63873 [==============================] - 5s 84us/step - loss: 0.2622 - acc: 0.8649\n",
      "Epoch 4/2000\n",
      "63873/63873 [==============================] - 6s 98us/step - loss: 0.2590 - acc: 0.8669\n",
      "Epoch 5/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.2554 - acc: 0.8689\n",
      "Epoch 6/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.2537 - acc: 0.8695\n",
      "Epoch 7/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.2509 - acc: 0.8717\n",
      "Epoch 8/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.2493 - acc: 0.8719\n",
      "Epoch 9/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.2475 - acc: 0.8740\n",
      "Epoch 10/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.2457 - acc: 0.8750\n",
      "Epoch 11/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.2442 - acc: 0.8757\n",
      "Epoch 12/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.2447 - acc: 0.8765\n",
      "Epoch 13/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.2424 - acc: 0.8768\n",
      "Epoch 14/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2400 - acc: 0.8792\n",
      "Epoch 15/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.2391 - acc: 0.8790\n",
      "Epoch 16/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.2402 - acc: 0.8790\n",
      "Epoch 17/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2365 - acc: 0.8812\n",
      "Epoch 18/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.2367 - acc: 0.8809\n",
      "Epoch 19/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.2337 - acc: 0.8840\n",
      "Epoch 20/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2319 - acc: 0.8849\n",
      "Epoch 21/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.2322 - acc: 0.8841\n",
      "Epoch 22/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.2309 - acc: 0.8858\n",
      "Epoch 23/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.2284 - acc: 0.8879\n",
      "Epoch 24/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.2267 - acc: 0.8872\n",
      "Epoch 25/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.2260 - acc: 0.8885\n",
      "Epoch 26/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.2245 - acc: 0.8887\n",
      "Epoch 27/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2231 - acc: 0.8913\n",
      "Epoch 28/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.2227 - acc: 0.8905\n",
      "Epoch 29/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.2208 - acc: 0.8915\n",
      "Epoch 30/2000\n",
      "63873/63873 [==============================] - 5s 86us/step - loss: 0.2206 - acc: 0.8913\n",
      "Epoch 31/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.2181 - acc: 0.8927\n",
      "Epoch 32/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.2197 - acc: 0.8922\n",
      "Epoch 33/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.2156 - acc: 0.8941\n",
      "Epoch 34/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.2153 - acc: 0.8942\n",
      "Epoch 35/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2132 - acc: 0.8967: 0s - loss: 0.2138 - acc: \n",
      "Epoch 36/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.2114 - acc: 0.8976\n",
      "Epoch 37/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.2094 - acc: 0.8977\n",
      "Epoch 38/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.2085 - acc: 0.8990\n",
      "Epoch 39/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.2075 - acc: 0.9009\n",
      "Epoch 40/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.2059 - acc: 0.9006\n",
      "Epoch 41/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.2047 - acc: 0.9017\n",
      "Epoch 42/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.2030 - acc: 0.9023\n",
      "Epoch 43/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.2015 - acc: 0.9035\n",
      "Epoch 44/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.1996 - acc: 0.9046\n",
      "Epoch 45/2000\n",
      "63873/63873 [==============================] - 6s 92us/step - loss: 0.1991 - acc: 0.9035\n",
      "Epoch 46/2000\n",
      "63873/63873 [==============================] - 7s 105us/step - loss: 0.1978 - acc: 0.9058\n",
      "Epoch 47/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.1959 - acc: 0.9058\n",
      "Epoch 48/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1955 - acc: 0.9062\n",
      "Epoch 49/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1939 - acc: 0.9070\n",
      "Epoch 50/2000\n",
      "63873/63873 [==============================] - 6s 95us/step - loss: 0.1920 - acc: 0.9088\n",
      "Epoch 51/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1929 - acc: 0.9083\n",
      "Epoch 52/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1911 - acc: 0.9086\n",
      "Epoch 53/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1896 - acc: 0.9090\n",
      "Epoch 54/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.1876 - acc: 0.9104\n",
      "Epoch 55/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1863 - acc: 0.9107\n",
      "Epoch 56/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1869 - acc: 0.9118\n",
      "Epoch 57/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.1847 - acc: 0.9118\n",
      "Epoch 58/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1854 - acc: 0.9114\n",
      "Epoch 59/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1851 - acc: 0.9110\n",
      "Epoch 60/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1799 - acc: 0.9151\n",
      "Epoch 61/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1799 - acc: 0.9138\n",
      "Epoch 62/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.1786 - acc: 0.9158\n",
      "Epoch 63/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.1773 - acc: 0.9159\n",
      "Epoch 64/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1766 - acc: 0.9169\n",
      "Epoch 65/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1751 - acc: 0.9179\n",
      "Epoch 66/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1755 - acc: 0.9172\n",
      "Epoch 67/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1723 - acc: 0.9189\n",
      "Epoch 68/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1707 - acc: 0.9200\n",
      "Epoch 69/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1716 - acc: 0.9193\n",
      "Epoch 70/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1680 - acc: 0.9206\n",
      "Epoch 71/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1681 - acc: 0.9201\n",
      "Epoch 72/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1668 - acc: 0.9220\n",
      "Epoch 73/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1659 - acc: 0.9225\n",
      "Epoch 74/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1636 - acc: 0.9231\n",
      "Epoch 75/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1671 - acc: 0.9222\n",
      "Epoch 76/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1625 - acc: 0.9237\n",
      "Epoch 77/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1611 - acc: 0.9248\n",
      "Epoch 78/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.1608 - acc: 0.9246\n",
      "Epoch 79/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1590 - acc: 0.9256\n",
      "Epoch 80/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1579 - acc: 0.9264\n",
      "Epoch 81/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1582 - acc: 0.9257\n",
      "Epoch 82/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1568 - acc: 0.9276\n",
      "Epoch 83/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1555 - acc: 0.9268\n",
      "Epoch 84/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1549 - acc: 0.9277\n",
      "Epoch 85/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.1546 - acc: 0.9294\n",
      "Epoch 86/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1558 - acc: 0.9273\n",
      "Epoch 87/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1524 - acc: 0.9297\n",
      "Epoch 88/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1495 - acc: 0.9304\n",
      "Epoch 89/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.1497 - acc: 0.9318\n",
      "Epoch 90/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1503 - acc: 0.9308\n",
      "Epoch 91/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1489 - acc: 0.9319\n",
      "Epoch 92/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1484 - acc: 0.9318\n",
      "Epoch 93/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1480 - acc: 0.9312\n",
      "Epoch 94/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.1478 - acc: 0.9320\n",
      "Epoch 95/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1456 - acc: 0.9336\n",
      "Epoch 96/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1459 - acc: 0.9338\n",
      "Epoch 97/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1439 - acc: 0.9342\n",
      "Epoch 98/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1445 - acc: 0.9334\n",
      "Epoch 99/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.1439 - acc: 0.9338\n",
      "Epoch 100/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.1423 - acc: 0.9341\n",
      "Epoch 101/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1420 - acc: 0.9348\n",
      "Epoch 102/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1422 - acc: 0.9355\n",
      "Epoch 103/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.1415 - acc: 0.9352\n",
      "Epoch 104/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1398 - acc: 0.9367\n",
      "Epoch 105/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1381 - acc: 0.9368\n",
      "Epoch 106/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1398 - acc: 0.9367\n",
      "Epoch 107/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1361 - acc: 0.9377\n",
      "Epoch 108/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1369 - acc: 0.9367\n",
      "Epoch 109/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1375 - acc: 0.9379\n",
      "Epoch 110/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1396 - acc: 0.9378\n",
      "Epoch 111/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.1362 - acc: 0.9379\n",
      "Epoch 112/2000\n",
      "63873/63873 [==============================] - 5s 76us/step - loss: 0.1338 - acc: 0.9401\n",
      "Epoch 113/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1348 - acc: 0.9398\n",
      "Epoch 114/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1343 - acc: 0.9385\n",
      "Epoch 115/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.1433 - acc: 0.9352\n",
      "Epoch 116/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1326 - acc: 0.9408\n",
      "Epoch 117/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1336 - acc: 0.9384\n",
      "Epoch 118/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1318 - acc: 0.9410\n",
      "Epoch 119/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1315 - acc: 0.9410\n",
      "Epoch 120/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1296 - acc: 0.9409\n",
      "Epoch 121/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1308 - acc: 0.9409\n",
      "Epoch 122/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1287 - acc: 0.9425\n",
      "Epoch 123/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1304 - acc: 0.9414\n",
      "Epoch 124/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.1301 - acc: 0.9421\n",
      "Epoch 125/2000\n",
      "63873/63873 [==============================] - 5s 78us/step - loss: 0.1274 - acc: 0.9436\n",
      "Epoch 126/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.1328 - acc: 0.9415\n",
      "Epoch 127/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1294 - acc: 0.9422\n",
      "Epoch 128/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1254 - acc: 0.9446\n",
      "Epoch 129/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1258 - acc: 0.9438\n",
      "Epoch 130/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1253 - acc: 0.9445\n",
      "Epoch 131/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1246 - acc: 0.9447\n",
      "Epoch 132/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1263 - acc: 0.9436\n",
      "Epoch 133/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.1221 - acc: 0.9460\n",
      "Epoch 134/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1243 - acc: 0.9449\n",
      "Epoch 135/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1238 - acc: 0.9452\n",
      "Epoch 136/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.1229 - acc: 0.9455\n",
      "Epoch 137/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1202 - acc: 0.9465\n",
      "Epoch 138/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1198 - acc: 0.9456\n",
      "Epoch 139/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.1204 - acc: 0.9475\n",
      "Epoch 140/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1191 - acc: 0.9470\n",
      "Epoch 141/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1208 - acc: 0.9467\n",
      "Epoch 142/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1172 - acc: 0.9472\n",
      "Epoch 143/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1186 - acc: 0.9476\n",
      "Epoch 144/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.1180 - acc: 0.9482\n",
      "Epoch 145/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.1149 - acc: 0.9487\n",
      "Epoch 146/2000\n",
      "63873/63873 [==============================] - 5s 78us/step - loss: 0.1187 - acc: 0.9469\n",
      "Epoch 147/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.1153 - acc: 0.9492\n",
      "Epoch 148/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.1152 - acc: 0.9488\n",
      "Epoch 149/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1150 - acc: 0.9490\n",
      "Epoch 150/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.1150 - acc: 0.9495\n",
      "Epoch 151/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1145 - acc: 0.9494\n",
      "Epoch 152/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1139 - acc: 0.9498\n",
      "Epoch 153/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1159 - acc: 0.9478\n",
      "Epoch 154/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1124 - acc: 0.9507\n",
      "Epoch 155/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1149 - acc: 0.9494\n",
      "Epoch 156/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.1129 - acc: 0.9509\n",
      "Epoch 157/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1110 - acc: 0.9518\n",
      "Epoch 158/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.1139 - acc: 0.9504\n",
      "Epoch 159/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 5s 79us/step - loss: 0.1104 - acc: 0.9511\n",
      "Epoch 160/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1133 - acc: 0.9500\n",
      "Epoch 161/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.1083 - acc: 0.9531\n",
      "Epoch 162/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1085 - acc: 0.9526\n",
      "Epoch 163/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.1096 - acc: 0.9520\n",
      "Epoch 164/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1091 - acc: 0.9514\n",
      "Epoch 165/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.1096 - acc: 0.9521\n",
      "Epoch 166/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.1079 - acc: 0.9530\n",
      "Epoch 167/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.1092 - acc: 0.9524\n",
      "Epoch 168/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.1065 - acc: 0.9537\n",
      "Epoch 169/2000\n",
      "63873/63873 [==============================] - 5s 75us/step - loss: 0.1081 - acc: 0.9524\n",
      "Epoch 170/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.1075 - acc: 0.9541\n",
      "Epoch 171/2000\n",
      "63873/63873 [==============================] - 6s 95us/step - loss: 0.1062 - acc: 0.9536\n",
      "Epoch 172/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.1052 - acc: 0.9537\n",
      "Epoch 173/2000\n",
      "63873/63873 [==============================] - 6s 90us/step - loss: 0.1070 - acc: 0.9530\n",
      "Epoch 174/2000\n",
      "63873/63873 [==============================] - 6s 95us/step - loss: 0.1031 - acc: 0.9546\n",
      "Epoch 175/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.1036 - acc: 0.9553\n",
      "Epoch 176/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.1042 - acc: 0.9552\n",
      "Epoch 177/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.1037 - acc: 0.9542\n",
      "Epoch 178/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.1058 - acc: 0.9538\n",
      "Epoch 179/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1008 - acc: 0.9560\n",
      "Epoch 180/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.1031 - acc: 0.9548\n",
      "Epoch 181/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.1022 - acc: 0.9553\n",
      "Epoch 182/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.1041 - acc: 0.9548\n",
      "Epoch 183/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1012 - acc: 0.9564\n",
      "Epoch 184/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.1009 - acc: 0.9559\n",
      "Epoch 185/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1005 - acc: 0.9563\n",
      "Epoch 186/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.1036 - acc: 0.9550\n",
      "Epoch 187/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.1014 - acc: 0.9559\n",
      "Epoch 188/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.1010 - acc: 0.9558\n",
      "Epoch 189/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0982 - acc: 0.9569\n",
      "Epoch 190/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0983 - acc: 0.9574\n",
      "Epoch 191/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0991 - acc: 0.9566\n",
      "Epoch 192/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0989 - acc: 0.9569\n",
      "Epoch 193/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0966 - acc: 0.9588\n",
      "Epoch 194/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0989 - acc: 0.9573\n",
      "Epoch 195/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0991 - acc: 0.9571\n",
      "Epoch 196/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0989 - acc: 0.9573\n",
      "Epoch 197/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0954 - acc: 0.9589\n",
      "Epoch 198/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0974 - acc: 0.9578\n",
      "Epoch 199/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0961 - acc: 0.9588\n",
      "Epoch 200/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0962 - acc: 0.9586\n",
      "Epoch 201/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0977 - acc: 0.9583\n",
      "Epoch 202/2000\n",
      "63873/63873 [==============================] - 5s 79us/step - loss: 0.0964 - acc: 0.9585\n",
      "Epoch 203/2000\n",
      "63873/63873 [==============================] - 5s 83us/step - loss: 0.0973 - acc: 0.9583\n",
      "Epoch 204/2000\n",
      "63873/63873 [==============================] - 6s 90us/step - loss: 0.0955 - acc: 0.9590\n",
      "Epoch 205/2000\n",
      "63873/63873 [==============================] - 5s 82us/step - loss: 0.0954 - acc: 0.9587\n",
      "Epoch 206/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.0942 - acc: 0.9602\n",
      "Epoch 207/2000\n",
      "63873/63873 [==============================] - 5s 75us/step - loss: 0.0959 - acc: 0.9586\n",
      "Epoch 208/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0948 - acc: 0.9593\n",
      "Epoch 209/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.0939 - acc: 0.9594\n",
      "Epoch 210/2000\n",
      "63873/63873 [==============================] - 5s 86us/step - loss: 0.0925 - acc: 0.9605\n",
      "Epoch 211/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0928 - acc: 0.9596\n",
      "Epoch 212/2000\n",
      "63873/63873 [==============================] - 5s 77us/step - loss: 0.0922 - acc: 0.9615\n",
      "Epoch 213/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.0920 - acc: 0.9606\n",
      "Epoch 214/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.0908 - acc: 0.9611\n",
      "Epoch 215/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0937 - acc: 0.9593\n",
      "Epoch 216/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0915 - acc: 0.9603\n",
      "Epoch 217/2000\n",
      "63873/63873 [==============================] - 5s 77us/step - loss: 0.0922 - acc: 0.9601\n",
      "Epoch 218/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.0933 - acc: 0.9604\n",
      "Epoch 219/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.0911 - acc: 0.9617\n",
      "Epoch 220/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0912 - acc: 0.9608\n",
      "Epoch 221/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0886 - acc: 0.9624\n",
      "Epoch 222/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0925 - acc: 0.9607\n",
      "Epoch 223/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0896 - acc: 0.9618\n",
      "Epoch 224/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0896 - acc: 0.9622\n",
      "Epoch 225/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0905 - acc: 0.9624\n",
      "Epoch 226/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0904 - acc: 0.9615\n",
      "Epoch 227/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0887 - acc: 0.9617\n",
      "Epoch 228/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0873 - acc: 0.9631\n",
      "Epoch 229/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0887 - acc: 0.9625\n",
      "Epoch 230/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0883 - acc: 0.9626\n",
      "Epoch 231/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0875 - acc: 0.9625: 1s - loss\n",
      "Epoch 232/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0870 - acc: 0.9624\n",
      "Epoch 233/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0864 - acc: 0.9636\n",
      "Epoch 234/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0889 - acc: 0.9618\n",
      "Epoch 235/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0863 - acc: 0.9635\n",
      "Epoch 236/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.0879 - acc: 0.9621\n",
      "Epoch 237/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0864 - acc: 0.9636\n",
      "Epoch 238/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0880 - acc: 0.9630\n",
      "Epoch 239/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0919 - acc: 0.9612\n",
      "Epoch 240/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0869 - acc: 0.9635\n",
      "Epoch 241/2000\n",
      "63873/63873 [==============================] - 5s 70us/step - loss: 0.0838 - acc: 0.9645\n",
      "Epoch 242/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0855 - acc: 0.9643\n",
      "Epoch 243/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0859 - acc: 0.9627\n",
      "Epoch 244/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.0843 - acc: 0.9643\n",
      "Epoch 245/2000\n",
      "63873/63873 [==============================] - 5s 82us/step - loss: 0.0840 - acc: 0.9641\n",
      "Epoch 246/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0831 - acc: 0.9646\n",
      "Epoch 247/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0850 - acc: 0.9635\n",
      "Epoch 248/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0859 - acc: 0.9635\n",
      "Epoch 249/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0821 - acc: 0.9649\n",
      "Epoch 250/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0827 - acc: 0.9647\n",
      "Epoch 251/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0822 - acc: 0.9650\n",
      "Epoch 252/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0866 - acc: 0.9638\n",
      "Epoch 253/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0812 - acc: 0.9662\n",
      "Epoch 254/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0830 - acc: 0.9652\n",
      "Epoch 255/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0818 - acc: 0.9650\n",
      "Epoch 256/2000\n",
      "63873/63873 [==============================] - 5s 75us/step - loss: 0.0822 - acc: 0.9656\n",
      "Epoch 257/2000\n",
      "63873/63873 [==============================] - 5s 76us/step - loss: 0.0813 - acc: 0.9656\n",
      "Epoch 258/2000\n",
      "63873/63873 [==============================] - 5s 82us/step - loss: 0.0820 - acc: 0.9651\n",
      "Epoch 259/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0827 - acc: 0.9648\n",
      "Epoch 260/2000\n",
      "63873/63873 [==============================] - 6s 94us/step - loss: 0.0798 - acc: 0.9659\n",
      "Epoch 261/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0833 - acc: 0.9642\n",
      "Epoch 262/2000\n",
      "63873/63873 [==============================] - 5s 85us/step - loss: 0.0796 - acc: 0.9667\n",
      "Epoch 263/2000\n",
      "63873/63873 [==============================] - 5s 75us/step - loss: 0.0799 - acc: 0.9664\n",
      "Epoch 264/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0831 - acc: 0.9656\n",
      "Epoch 265/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0780 - acc: 0.9669\n",
      "Epoch 266/2000\n",
      "63873/63873 [==============================] - 5s 79us/step - loss: 0.0840 - acc: 0.9645\n",
      "Epoch 267/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0783 - acc: 0.9666\n",
      "Epoch 268/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0823 - acc: 0.9656\n",
      "Epoch 269/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0774 - acc: 0.9675\n",
      "Epoch 270/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0789 - acc: 0.9666\n",
      "Epoch 271/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0813 - acc: 0.9660\n",
      "Epoch 272/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0805 - acc: 0.9659\n",
      "Epoch 273/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0803 - acc: 0.9659\n",
      "Epoch 274/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0794 - acc: 0.9661: 0s - loss: 0.0799 - a\n",
      "Epoch 275/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0775 - acc: 0.9678\n",
      "Epoch 276/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0765 - acc: 0.9679\n",
      "Epoch 277/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0796 - acc: 0.9669\n",
      "Epoch 278/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0767 - acc: 0.9680\n",
      "Epoch 279/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0774 - acc: 0.9674\n",
      "Epoch 280/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0762 - acc: 0.9685\n",
      "Epoch 281/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0774 - acc: 0.9673\n",
      "Epoch 282/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0815 - acc: 0.9660\n",
      "Epoch 283/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0780 - acc: 0.9679\n",
      "Epoch 284/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0794 - acc: 0.9671\n",
      "Epoch 285/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0769 - acc: 0.9671\n",
      "Epoch 286/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0762 - acc: 0.9680\n",
      "Epoch 287/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0780 - acc: 0.9674\n",
      "Epoch 288/2000\n",
      "63873/63873 [==============================] - 5s 82us/step - loss: 0.0770 - acc: 0.9680\n",
      "Epoch 289/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0758 - acc: 0.9680\n",
      "Epoch 290/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0769 - acc: 0.9677\n",
      "Epoch 291/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0785 - acc: 0.9672\n",
      "Epoch 292/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0757 - acc: 0.9682\n",
      "Epoch 293/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0764 - acc: 0.9680\n",
      "Epoch 294/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0776 - acc: 0.9676\n",
      "Epoch 295/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0756 - acc: 0.9686\n",
      "Epoch 296/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0742 - acc: 0.9691\n",
      "Epoch 297/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0749 - acc: 0.9682\n",
      "Epoch 298/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0753 - acc: 0.9686\n",
      "Epoch 299/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0759 - acc: 0.9684\n",
      "Epoch 300/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0722 - acc: 0.9702\n",
      "Epoch 301/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0827 - acc: 0.9650\n",
      "Epoch 302/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0723 - acc: 0.9695\n",
      "Epoch 303/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0728 - acc: 0.9697\n",
      "Epoch 304/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0736 - acc: 0.9695\n",
      "Epoch 305/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0709 - acc: 0.9703\n",
      "Epoch 306/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0752 - acc: 0.9684\n",
      "Epoch 307/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0728 - acc: 0.9691\n",
      "Epoch 308/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0742 - acc: 0.9697\n",
      "Epoch 309/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0710 - acc: 0.9704\n",
      "Epoch 310/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0737 - acc: 0.9697\n",
      "Epoch 311/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0729 - acc: 0.9702\n",
      "Epoch 312/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0741 - acc: 0.9696\n",
      "Epoch 313/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0728 - acc: 0.9696\n",
      "Epoch 314/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0716 - acc: 0.9706\n",
      "Epoch 315/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0708 - acc: 0.9701\n",
      "Epoch 316/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0702 - acc: 0.9704\n",
      "Epoch 317/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.0703 - acc: 0.9700\n",
      "Epoch 318/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0719 - acc: 0.9704\n",
      "Epoch 319/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0706 - acc: 0.9704\n",
      "Epoch 320/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0713 - acc: 0.9716\n",
      "Epoch 321/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0720 - acc: 0.9703\n",
      "Epoch 322/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0713 - acc: 0.9708\n",
      "Epoch 323/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0692 - acc: 0.9713\n",
      "Epoch 324/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0711 - acc: 0.9700\n",
      "Epoch 325/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0732 - acc: 0.9697\n",
      "Epoch 326/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0721 - acc: 0.9701\n",
      "Epoch 327/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0695 - acc: 0.9707\n",
      "Epoch 328/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0723 - acc: 0.9697\n",
      "Epoch 329/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0717 - acc: 0.9699\n",
      "Epoch 330/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0719 - acc: 0.9696\n",
      "Epoch 331/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0685 - acc: 0.9711\n",
      "Epoch 332/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0721 - acc: 0.9699\n",
      "Epoch 333/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0697 - acc: 0.9710\n",
      "Epoch 334/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0662 - acc: 0.9726\n",
      "Epoch 335/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0694 - acc: 0.9713\n",
      "Epoch 336/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0713 - acc: 0.9704\n",
      "Epoch 337/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0681 - acc: 0.9720\n",
      "Epoch 338/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0698 - acc: 0.9707\n",
      "Epoch 339/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0686 - acc: 0.9722\n",
      "Epoch 340/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0679 - acc: 0.9723\n",
      "Epoch 341/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0714 - acc: 0.9709\n",
      "Epoch 342/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0682 - acc: 0.9717\n",
      "Epoch 343/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0685 - acc: 0.9717\n",
      "Epoch 344/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0722 - acc: 0.9711\n",
      "Epoch 345/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0681 - acc: 0.9721\n",
      "Epoch 346/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0708 - acc: 0.9706\n",
      "Epoch 347/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0697 - acc: 0.9712\n",
      "Epoch 348/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0692 - acc: 0.9714\n",
      "Epoch 349/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0668 - acc: 0.9725\n",
      "Epoch 350/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0697 - acc: 0.9711\n",
      "Epoch 351/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0663 - acc: 0.9729\n",
      "Epoch 352/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0671 - acc: 0.9717\n",
      "Epoch 353/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0670 - acc: 0.9723\n",
      "Epoch 354/2000\n",
      "63873/63873 [==============================] - 5s 80us/step - loss: 0.0698 - acc: 0.9714\n",
      "Epoch 355/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0684 - acc: 0.9717\n",
      "Epoch 356/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0670 - acc: 0.9728\n",
      "Epoch 357/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.0651 - acc: 0.9733\n",
      "Epoch 358/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0668 - acc: 0.9719\n",
      "Epoch 359/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0662 - acc: 0.9734\n",
      "Epoch 360/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0695 - acc: 0.9719\n",
      "Epoch 361/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0669 - acc: 0.9725\n",
      "Epoch 362/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0669 - acc: 0.9724\n",
      "Epoch 363/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0665 - acc: 0.9722\n",
      "Epoch 364/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0663 - acc: 0.9733\n",
      "Epoch 365/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0686 - acc: 0.9719\n",
      "Epoch 366/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0655 - acc: 0.9732\n",
      "Epoch 367/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0664 - acc: 0.9733\n",
      "Epoch 368/2000\n",
      "63873/63873 [==============================] - 7s 106us/step - loss: 0.0672 - acc: 0.9728\n",
      "Epoch 369/2000\n",
      "63873/63873 [==============================] - 5s 81us/step - loss: 0.0669 - acc: 0.9728\n",
      "Epoch 370/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0653 - acc: 0.9734\n",
      "Epoch 371/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0657 - acc: 0.9727\n",
      "Epoch 372/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0665 - acc: 0.9734\n",
      "Epoch 373/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0677 - acc: 0.9721\n",
      "Epoch 374/2000\n",
      "63873/63873 [==============================] - 5s 79us/step - loss: 0.0677 - acc: 0.9721\n",
      "Epoch 375/2000\n",
      "63873/63873 [==============================] - 5s 83us/step - loss: 0.0649 - acc: 0.9737\n",
      "Epoch 376/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0643 - acc: 0.9732\n",
      "Epoch 377/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0632 - acc: 0.9743\n",
      "Epoch 378/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0654 - acc: 0.9732\n",
      "Epoch 379/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0645 - acc: 0.9738\n",
      "Epoch 380/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0637 - acc: 0.9737\n",
      "Epoch 381/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0639 - acc: 0.9735\n",
      "Epoch 382/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0643 - acc: 0.9737\n",
      "Epoch 383/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0631 - acc: 0.9742\n",
      "Epoch 384/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0612 - acc: 0.9751\n",
      "Epoch 385/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0633 - acc: 0.9738\n",
      "Epoch 386/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0642 - acc: 0.9738\n",
      "Epoch 387/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0629 - acc: 0.9744\n",
      "Epoch 388/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0642 - acc: 0.9742\n",
      "Epoch 389/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0655 - acc: 0.9739\n",
      "Epoch 390/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0654 - acc: 0.9736\n",
      "Epoch 391/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0614 - acc: 0.9754\n",
      "Epoch 392/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0683 - acc: 0.9725\n",
      "Epoch 393/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0646 - acc: 0.9738\n",
      "Epoch 394/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0634 - acc: 0.9739\n",
      "Epoch 395/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0782 - acc: 0.9688\n",
      "Epoch 396/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0651 - acc: 0.9740\n",
      "Epoch 397/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0624 - acc: 0.9748\n",
      "Epoch 398/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0614 - acc: 0.9753\n",
      "Epoch 399/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0637 - acc: 0.9746\n",
      "Epoch 400/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0610 - acc: 0.9749\n",
      "Epoch 401/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0585 - acc: 0.9764\n",
      "Epoch 402/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0582 - acc: 0.9763\n",
      "Epoch 403/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0630 - acc: 0.9747\n",
      "Epoch 404/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0598 - acc: 0.9756\n",
      "Epoch 405/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0606 - acc: 0.9753\n",
      "Epoch 406/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0625 - acc: 0.9753\n",
      "Epoch 407/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0627 - acc: 0.9746\n",
      "Epoch 408/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0598 - acc: 0.9763\n",
      "Epoch 409/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0605 - acc: 0.9758\n",
      "Epoch 410/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0613 - acc: 0.9752\n",
      "Epoch 411/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0618 - acc: 0.9751\n",
      "Epoch 412/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0594 - acc: 0.9760\n",
      "Epoch 413/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0591 - acc: 0.9759\n",
      "Epoch 414/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0617 - acc: 0.9750\n",
      "Epoch 415/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0617 - acc: 0.9750\n",
      "Epoch 416/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0602 - acc: 0.9759\n",
      "Epoch 417/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0601 - acc: 0.9755\n",
      "Epoch 418/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0646 - acc: 0.9747\n",
      "Epoch 419/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0611 - acc: 0.9746\n",
      "Epoch 420/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0618 - acc: 0.9756\n",
      "Epoch 421/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0593 - acc: 0.9764\n",
      "Epoch 422/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0616 - acc: 0.9754\n",
      "Epoch 423/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0600 - acc: 0.9760\n",
      "Epoch 424/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0601 - acc: 0.9757\n",
      "Epoch 425/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0581 - acc: 0.9765\n",
      "Epoch 426/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0597 - acc: 0.9754\n",
      "Epoch 427/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0596 - acc: 0.9768\n",
      "Epoch 428/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0610 - acc: 0.9759: 2s - loss: - ETA: 1\n",
      "Epoch 429/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0616 - acc: 0.9752\n",
      "Epoch 430/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0597 - acc: 0.9765\n",
      "Epoch 431/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0624 - acc: 0.9757\n",
      "Epoch 432/2000\n",
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0594 - acc: 0.9765\n",
      "Epoch 433/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0575 - acc: 0.9778\n",
      "Epoch 434/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0606 - acc: 0.9763\n",
      "Epoch 435/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0607 - acc: 0.9765\n",
      "Epoch 436/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0572 - acc: 0.9777\n",
      "Epoch 437/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0576 - acc: 0.9772\n",
      "Epoch 438/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0567 - acc: 0.9776\n",
      "Epoch 439/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0627 - acc: 0.9754\n",
      "Epoch 440/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0554 - acc: 0.9778\n",
      "Epoch 441/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0562 - acc: 0.9779\n",
      "Epoch 442/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0599 - acc: 0.9765\n",
      "Epoch 443/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0566 - acc: 0.9769\n",
      "Epoch 444/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0565 - acc: 0.9777\n",
      "Epoch 445/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0613 - acc: 0.9756\n",
      "Epoch 446/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0567 - acc: 0.9774\n",
      "Epoch 447/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0549 - acc: 0.9782\n",
      "Epoch 448/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0627 - acc: 0.9752\n",
      "Epoch 449/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0599 - acc: 0.9760\n",
      "Epoch 450/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0569 - acc: 0.9772\n",
      "Epoch 451/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0557 - acc: 0.9778\n",
      "Epoch 452/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0555 - acc: 0.9783\n",
      "Epoch 453/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0564 - acc: 0.9781\n",
      "Epoch 454/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0561 - acc: 0.9778\n",
      "Epoch 455/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0588 - acc: 0.9763\n",
      "Epoch 456/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0582 - acc: 0.9773\n",
      "Epoch 457/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0617 - acc: 0.9762\n",
      "Epoch 458/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0558 - acc: 0.9781\n",
      "Epoch 459/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0555 - acc: 0.9775\n",
      "Epoch 460/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0582 - acc: 0.9772\n",
      "Epoch 461/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0546 - acc: 0.9786\n",
      "Epoch 462/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0564 - acc: 0.9784\n",
      "Epoch 463/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0551 - acc: 0.9783\n",
      "Epoch 464/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0559 - acc: 0.9784\n",
      "Epoch 465/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0577 - acc: 0.9772\n",
      "Epoch 466/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0559 - acc: 0.9779\n",
      "Epoch 467/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0550 - acc: 0.9786\n",
      "Epoch 468/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0527 - acc: 0.9786\n",
      "Epoch 469/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0545 - acc: 0.9781\n",
      "Epoch 470/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0561 - acc: 0.9785\n",
      "Epoch 471/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0567 - acc: 0.9777\n",
      "Epoch 472/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0544 - acc: 0.9785\n",
      "Epoch 473/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0516 - acc: 0.9805\n",
      "Epoch 474/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0576 - acc: 0.9776\n",
      "Epoch 475/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0547 - acc: 0.9789\n",
      "Epoch 476/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0529 - acc: 0.9800\n",
      "Epoch 477/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0545 - acc: 0.9784\n",
      "Epoch 478/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0543 - acc: 0.9786\n",
      "Epoch 479/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0513 - acc: 0.9798\n",
      "Epoch 480/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0519 - acc: 0.9797\n",
      "Epoch 481/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0537 - acc: 0.9794\n",
      "Epoch 482/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0551 - acc: 0.9784\n",
      "Epoch 483/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0533 - acc: 0.9790\n",
      "Epoch 484/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0547 - acc: 0.9782\n",
      "Epoch 485/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0539 - acc: 0.9795\n",
      "Epoch 486/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0533 - acc: 0.9794\n",
      "Epoch 487/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0530 - acc: 0.9793\n",
      "Epoch 488/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0588 - acc: 0.9780\n",
      "Epoch 489/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0518 - acc: 0.9798\n",
      "Epoch 490/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0533 - acc: 0.9791\n",
      "Epoch 491/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0552 - acc: 0.9786\n",
      "Epoch 492/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0510 - acc: 0.9796\n",
      "Epoch 493/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0509 - acc: 0.9799\n",
      "Epoch 494/2000\n",
      "63873/63873 [==============================] - 5s 72us/step - loss: 0.0520 - acc: 0.9794\n",
      "Epoch 495/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0648 - acc: 0.9744\n",
      "Epoch 496/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0509 - acc: 0.9801\n",
      "Epoch 497/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0510 - acc: 0.9805\n",
      "Epoch 498/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0504 - acc: 0.9806\n",
      "Epoch 499/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0573 - acc: 0.9773\n",
      "Epoch 500/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0496 - acc: 0.9808\n",
      "Epoch 501/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0518 - acc: 0.9795\n",
      "Epoch 502/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.0524 - acc: 0.9795\n",
      "Epoch 503/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0496 - acc: 0.9806\n",
      "Epoch 504/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0537 - acc: 0.9791\n",
      "Epoch 505/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0515 - acc: 0.9805\n",
      "Epoch 506/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0524 - acc: 0.9803\n",
      "Epoch 507/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0538 - acc: 0.9796\n",
      "Epoch 508/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0510 - acc: 0.9800\n",
      "Epoch 509/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0515 - acc: 0.9801\n",
      "Epoch 510/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0525 - acc: 0.9802\n",
      "Epoch 511/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0545 - acc: 0.9796\n",
      "Epoch 512/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0504 - acc: 0.9805\n",
      "Epoch 513/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0501 - acc: 0.9810\n",
      "Epoch 514/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0518 - acc: 0.9803\n",
      "Epoch 515/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0531 - acc: 0.9795\n",
      "Epoch 516/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0518 - acc: 0.9803\n",
      "Epoch 517/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0510 - acc: 0.9803\n",
      "Epoch 518/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0527 - acc: 0.9796\n",
      "Epoch 519/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0518 - acc: 0.9800\n",
      "Epoch 520/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0514 - acc: 0.9802\n",
      "Epoch 521/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0488 - acc: 0.9809\n",
      "Epoch 522/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0479 - acc: 0.9816\n",
      "Epoch 523/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0510 - acc: 0.9801\n",
      "Epoch 524/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0480 - acc: 0.9813\n",
      "Epoch 525/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0512 - acc: 0.9801\n",
      "Epoch 526/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0506 - acc: 0.9800\n",
      "Epoch 527/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0511 - acc: 0.9803\n",
      "Epoch 528/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0462 - acc: 0.9820\n",
      "Epoch 529/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0502 - acc: 0.9808\n",
      "Epoch 530/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0507 - acc: 0.9809\n",
      "Epoch 531/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0483 - acc: 0.9815\n",
      "Epoch 532/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0495 - acc: 0.9810\n",
      "Epoch 533/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0549 - acc: 0.9791\n",
      "Epoch 534/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0489 - acc: 0.9805\n",
      "Epoch 535/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0488 - acc: 0.9814\n",
      "Epoch 536/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0495 - acc: 0.9816\n",
      "Epoch 537/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0494 - acc: 0.9812\n",
      "Epoch 538/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0498 - acc: 0.9814\n",
      "Epoch 539/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0514 - acc: 0.9803\n",
      "Epoch 540/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0508 - acc: 0.9808\n",
      "Epoch 541/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0467 - acc: 0.9825\n",
      "Epoch 542/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0498 - acc: 0.9815\n",
      "Epoch 543/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0479 - acc: 0.9820\n",
      "Epoch 544/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0492 - acc: 0.9814\n",
      "Epoch 545/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0512 - acc: 0.9807\n",
      "Epoch 546/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0483 - acc: 0.9810\n",
      "Epoch 547/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0451 - acc: 0.9831\n",
      "Epoch 548/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0477 - acc: 0.9820\n",
      "Epoch 549/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0486 - acc: 0.9812\n",
      "Epoch 550/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0474 - acc: 0.9816\n",
      "Epoch 551/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0481 - acc: 0.9816\n",
      "Epoch 552/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0488 - acc: 0.9817\n",
      "Epoch 553/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0454 - acc: 0.9831\n",
      "Epoch 554/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0484 - acc: 0.9814\n",
      "Epoch 555/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0460 - acc: 0.9825\n",
      "Epoch 556/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0496 - acc: 0.9816\n",
      "Epoch 557/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0490 - acc: 0.9815\n",
      "Epoch 558/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0478 - acc: 0.9817\n",
      "Epoch 559/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0468 - acc: 0.9823\n",
      "Epoch 560/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0505 - acc: 0.9812\n",
      "Epoch 561/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0500 - acc: 0.9810\n",
      "Epoch 562/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0487 - acc: 0.9818\n",
      "Epoch 563/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0468 - acc: 0.9828\n",
      "Epoch 564/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0474 - acc: 0.9821\n",
      "Epoch 565/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0466 - acc: 0.9827\n",
      "Epoch 566/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0469 - acc: 0.9820\n",
      "Epoch 567/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0468 - acc: 0.9821\n",
      "Epoch 568/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0477 - acc: 0.9817\n",
      "Epoch 569/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0440 - acc: 0.9840\n",
      "Epoch 570/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0501 - acc: 0.9812\n",
      "Epoch 571/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0466 - acc: 0.9822\n",
      "Epoch 572/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0490 - acc: 0.9820\n",
      "Epoch 573/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0458 - acc: 0.9823\n",
      "Epoch 574/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0463 - acc: 0.9828\n",
      "Epoch 575/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0439 - acc: 0.9837\n",
      "Epoch 576/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0444 - acc: 0.9835\n",
      "Epoch 577/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0458 - acc: 0.9830\n",
      "Epoch 578/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0454 - acc: 0.9831\n",
      "Epoch 579/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0513 - acc: 0.9805\n",
      "Epoch 580/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0485 - acc: 0.9815\n",
      "Epoch 581/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0479 - acc: 0.9822\n",
      "Epoch 582/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0452 - acc: 0.9828\n",
      "Epoch 583/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0465 - acc: 0.9824\n",
      "Epoch 584/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0454 - acc: 0.9831\n",
      "Epoch 585/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0495 - acc: 0.9817\n",
      "Epoch 586/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0450 - acc: 0.9832\n",
      "Epoch 587/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0450 - acc: 0.9831\n",
      "Epoch 588/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0456 - acc: 0.9830\n",
      "Epoch 589/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0490 - acc: 0.9818\n",
      "Epoch 590/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0443 - acc: 0.9837\n",
      "Epoch 591/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0435 - acc: 0.9839\n",
      "Epoch 592/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0464 - acc: 0.9822\n",
      "Epoch 593/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0478 - acc: 0.9823\n",
      "Epoch 594/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0458 - acc: 0.9827\n",
      "Epoch 595/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0448 - acc: 0.9836: 0s - loss: 0.0446 - acc: 0.98\n",
      "Epoch 596/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0484 - acc: 0.9820\n",
      "Epoch 597/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0437 - acc: 0.9838\n",
      "Epoch 598/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0431 - acc: 0.9834\n",
      "Epoch 599/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0445 - acc: 0.9837\n",
      "Epoch 600/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0464 - acc: 0.9826: 0s - loss: 0.046\n",
      "Epoch 601/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0474 - acc: 0.9827\n",
      "Epoch 602/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0449 - acc: 0.9833\n",
      "Epoch 603/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0475 - acc: 0.9823\n",
      "Epoch 604/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0448 - acc: 0.9832\n",
      "Epoch 605/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0439 - acc: 0.9834\n",
      "Epoch 606/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0438 - acc: 0.9836\n",
      "Epoch 607/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0446 - acc: 0.9836\n",
      "Epoch 608/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0443 - acc: 0.9836\n",
      "Epoch 609/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0458 - acc: 0.9832\n",
      "Epoch 610/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0472 - acc: 0.9831\n",
      "Epoch 611/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0433 - acc: 0.9838: 1\n",
      "Epoch 612/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0428 - acc: 0.9838\n",
      "Epoch 613/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0459 - acc: 0.9831\n",
      "Epoch 614/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0461 - acc: 0.9829\n",
      "Epoch 615/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0439 - acc: 0.9837\n",
      "Epoch 616/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0456 - acc: 0.9835\n",
      "Epoch 617/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0439 - acc: 0.9839\n",
      "Epoch 618/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0479 - acc: 0.9825\n",
      "Epoch 619/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0405 - acc: 0.9852\n",
      "Epoch 620/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0436 - acc: 0.9840: 3s - lo\n",
      "Epoch 621/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0445 - acc: 0.9835\n",
      "Epoch 622/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0438 - acc: 0.9832\n",
      "Epoch 623/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0430 - acc: 0.9840\n",
      "Epoch 624/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0437 - acc: 0.9840\n",
      "Epoch 625/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0445 - acc: 0.9839\n",
      "Epoch 626/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0427 - acc: 0.9841\n",
      "Epoch 627/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0417 - acc: 0.9842\n",
      "Epoch 628/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0474 - acc: 0.9830\n",
      "Epoch 629/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0444 - acc: 0.9833\n",
      "Epoch 630/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0423 - acc: 0.9847\n",
      "Epoch 631/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0433 - acc: 0.9841\n",
      "Epoch 632/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0447 - acc: 0.9834\n",
      "Epoch 633/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0428 - acc: 0.9841\n",
      "Epoch 634/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0426 - acc: 0.9841\n",
      "Epoch 635/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0426 - acc: 0.9842\n",
      "Epoch 636/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0431 - acc: 0.9839\n",
      "Epoch 637/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0470 - acc: 0.9831\n",
      "Epoch 638/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0457 - acc: 0.9830\n",
      "Epoch 639/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0436 - acc: 0.9835\n",
      "Epoch 640/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0426 - acc: 0.9844\n",
      "Epoch 641/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0526 - acc: 0.9806\n",
      "Epoch 642/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0404 - acc: 0.9846\n",
      "Epoch 643/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0450 - acc: 0.9833\n",
      "Epoch 644/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0406 - acc: 0.9855: 0s - loss: 0.0405 - acc: 0.\n",
      "Epoch 645/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0426 - acc: 0.9848\n",
      "Epoch 646/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0443 - acc: 0.9836\n",
      "Epoch 647/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0464 - acc: 0.9827\n",
      "Epoch 648/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0457 - acc: 0.9832\n",
      "Epoch 649/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0416 - acc: 0.9848\n",
      "Epoch 650/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0425 - acc: 0.9844\n",
      "Epoch 651/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0419 - acc: 0.9844\n",
      "Epoch 652/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0453 - acc: 0.9841\n",
      "Epoch 653/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0411 - acc: 0.9854\n",
      "Epoch 654/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0401 - acc: 0.9856\n",
      "Epoch 655/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0405 - acc: 0.9855\n",
      "Epoch 656/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0421 - acc: 0.9853\n",
      "Epoch 657/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0431 - acc: 0.9839\n",
      "Epoch 658/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0450 - acc: 0.9837\n",
      "Epoch 659/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0434 - acc: 0.9843\n",
      "Epoch 660/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0410 - acc: 0.9847\n",
      "Epoch 661/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0438 - acc: 0.9843\n",
      "Epoch 662/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0447 - acc: 0.9840\n",
      "Epoch 663/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0420 - acc: 0.9849\n",
      "Epoch 664/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0387 - acc: 0.9858\n",
      "Epoch 665/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0410 - acc: 0.9856\n",
      "Epoch 666/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0440 - acc: 0.9843\n",
      "Epoch 667/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0401 - acc: 0.9855\n",
      "Epoch 668/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0417 - acc: 0.9845\n",
      "Epoch 669/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0404 - acc: 0.9852\n",
      "Epoch 670/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0402 - acc: 0.9852\n",
      "Epoch 671/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0402 - acc: 0.9849\n",
      "Epoch 672/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0435 - acc: 0.9845\n",
      "Epoch 673/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0453 - acc: 0.9835\n",
      "Epoch 674/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0385 - acc: 0.9862\n",
      "Epoch 675/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0413 - acc: 0.9849\n",
      "Epoch 676/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0413 - acc: 0.9849\n",
      "Epoch 677/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0405 - acc: 0.9856\n",
      "Epoch 678/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0494 - acc: 0.9825\n",
      "Epoch 679/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0381 - acc: 0.9865\n",
      "Epoch 680/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0395 - acc: 0.9855\n",
      "Epoch 681/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0429 - acc: 0.9845\n",
      "Epoch 682/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0394 - acc: 0.9856\n",
      "Epoch 683/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0397 - acc: 0.9855\n",
      "Epoch 684/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0407 - acc: 0.9852\n",
      "Epoch 685/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0391 - acc: 0.9860\n",
      "Epoch 686/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0407 - acc: 0.9853\n",
      "Epoch 687/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0388 - acc: 0.9863\n",
      "Epoch 688/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0422 - acc: 0.9848\n",
      "Epoch 689/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0387 - acc: 0.9864\n",
      "Epoch 690/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0462 - acc: 0.9830\n",
      "Epoch 691/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0416 - acc: 0.9855\n",
      "Epoch 692/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0411 - acc: 0.9852\n",
      "Epoch 693/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0396 - acc: 0.9861\n",
      "Epoch 694/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0403 - acc: 0.9854\n",
      "Epoch 695/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0383 - acc: 0.9861\n",
      "Epoch 696/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0399 - acc: 0.9857\n",
      "Epoch 697/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0398 - acc: 0.9855\n",
      "Epoch 698/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0408 - acc: 0.9852\n",
      "Epoch 699/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0400 - acc: 0.9858\n",
      "Epoch 700/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0400 - acc: 0.9857\n",
      "Epoch 701/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0446 - acc: 0.9841\n",
      "Epoch 702/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0388 - acc: 0.9859\n",
      "Epoch 703/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0394 - acc: 0.9858\n",
      "Epoch 704/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0384 - acc: 0.9862\n",
      "Epoch 705/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0391 - acc: 0.9860\n",
      "Epoch 706/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0400 - acc: 0.9856\n",
      "Epoch 707/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0420 - acc: 0.9850\n",
      "Epoch 708/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0409 - acc: 0.9855\n",
      "Epoch 709/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0397 - acc: 0.9858\n",
      "Epoch 710/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0413 - acc: 0.9853\n",
      "Epoch 711/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0366 - acc: 0.9870\n",
      "Epoch 712/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0399 - acc: 0.9855\n",
      "Epoch 713/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0412 - acc: 0.9854\n",
      "Epoch 714/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0391 - acc: 0.9857\n",
      "Epoch 715/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0416 - acc: 0.9855\n",
      "Epoch 716/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0391 - acc: 0.9862\n",
      "Epoch 717/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0410 - acc: 0.9857\n",
      "Epoch 718/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0391 - acc: 0.9863\n",
      "Epoch 719/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0419 - acc: 0.9852\n",
      "Epoch 720/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0384 - acc: 0.9865\n",
      "Epoch 721/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0391 - acc: 0.9859\n",
      "Epoch 722/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0413 - acc: 0.9851\n",
      "Epoch 723/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0357 - acc: 0.9872\n",
      "Epoch 724/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0408 - acc: 0.9851\n",
      "Epoch 725/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0399 - acc: 0.9854\n",
      "Epoch 726/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0380 - acc: 0.9868\n",
      "Epoch 727/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0428 - acc: 0.9849\n",
      "Epoch 728/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0393 - acc: 0.9855\n",
      "Epoch 729/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0383 - acc: 0.9867\n",
      "Epoch 730/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0395 - acc: 0.9859\n",
      "Epoch 731/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0389 - acc: 0.9859\n",
      "Epoch 732/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0401 - acc: 0.9852\n",
      "Epoch 733/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0368 - acc: 0.9864\n",
      "Epoch 734/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0365 - acc: 0.9867\n",
      "Epoch 735/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0351 - acc: 0.9874\n",
      "Epoch 736/2000\n",
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0382 - acc: 0.9864: 1s - loss:\n",
      "Epoch 737/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0417 - acc: 0.9846\n",
      "Epoch 738/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0374 - acc: 0.9868\n",
      "Epoch 739/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0389 - acc: 0.9861\n",
      "Epoch 740/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0385 - acc: 0.9866\n",
      "Epoch 741/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0395 - acc: 0.9859\n",
      "Epoch 742/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0392 - acc: 0.9862\n",
      "Epoch 743/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0394 - acc: 0.9861\n",
      "Epoch 744/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0357 - acc: 0.9873\n",
      "Epoch 745/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0384 - acc: 0.9865\n",
      "Epoch 746/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0373 - acc: 0.9865\n",
      "Epoch 747/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0389 - acc: 0.9856\n",
      "Epoch 748/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0373 - acc: 0.9870\n",
      "Epoch 749/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0389 - acc: 0.9862\n",
      "Epoch 750/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0372 - acc: 0.9871\n",
      "Epoch 751/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0358 - acc: 0.9874\n",
      "Epoch 752/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0372 - acc: 0.9869\n",
      "Epoch 753/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0391 - acc: 0.9861\n",
      "Epoch 754/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0366 - acc: 0.9868\n",
      "Epoch 755/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0384 - acc: 0.9862\n",
      "Epoch 756/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0371 - acc: 0.9869\n",
      "Epoch 757/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0372 - acc: 0.9871\n",
      "Epoch 758/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0374 - acc: 0.9867\n",
      "Epoch 759/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0406 - acc: 0.9856\n",
      "Epoch 760/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0363 - acc: 0.9872\n",
      "Epoch 761/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0387 - acc: 0.9861\n",
      "Epoch 762/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0368 - acc: 0.9870\n",
      "Epoch 763/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0362 - acc: 0.9873\n",
      "Epoch 764/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0359 - acc: 0.9873\n",
      "Epoch 765/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0367 - acc: 0.9869\n",
      "Epoch 766/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0367 - acc: 0.9869\n",
      "Epoch 767/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0384 - acc: 0.9859\n",
      "Epoch 768/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0375 - acc: 0.9870\n",
      "Epoch 769/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0357 - acc: 0.9877\n",
      "Epoch 770/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0354 - acc: 0.9877\n",
      "Epoch 771/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0370 - acc: 0.9871: 0s - loss: 0.0371 - acc: 0.98\n",
      "Epoch 772/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0364 - acc: 0.9868\n",
      "Epoch 773/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0402 - acc: 0.9862\n",
      "Epoch 774/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0420 - acc: 0.9850\n",
      "Epoch 775/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0361 - acc: 0.9877\n",
      "Epoch 776/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0401 - acc: 0.9860\n",
      "Epoch 777/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0351 - acc: 0.9877\n",
      "Epoch 778/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0353 - acc: 0.9877\n",
      "Epoch 779/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0343 - acc: 0.9886\n",
      "Epoch 780/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0400 - acc: 0.9863\n",
      "Epoch 781/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0362 - acc: 0.9871\n",
      "Epoch 782/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0342 - acc: 0.9882\n",
      "Epoch 783/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0364 - acc: 0.9867\n",
      "Epoch 784/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0339 - acc: 0.9878\n",
      "Epoch 785/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0365 - acc: 0.9871\n",
      "Epoch 786/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0364 - acc: 0.9874\n",
      "Epoch 787/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0371 - acc: 0.9871\n",
      "Epoch 788/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0390 - acc: 0.9862\n",
      "Epoch 789/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0368 - acc: 0.9872\n",
      "Epoch 790/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0375 - acc: 0.9868\n",
      "Epoch 791/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0343 - acc: 0.9878\n",
      "Epoch 792/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0375 - acc: 0.9867\n",
      "Epoch 793/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0350 - acc: 0.9873\n",
      "Epoch 794/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0358 - acc: 0.9874\n",
      "Epoch 795/2000\n",
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0330 - acc: 0.9887\n",
      "Epoch 796/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0360 - acc: 0.9880\n",
      "Epoch 797/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0396 - acc: 0.9863\n",
      "Epoch 798/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0370 - acc: 0.9873\n",
      "Epoch 799/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0385 - acc: 0.9866\n",
      "Epoch 800/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0344 - acc: 0.9883\n",
      "Epoch 801/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0365 - acc: 0.9874\n",
      "Epoch 802/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0345 - acc: 0.9876\n",
      "Epoch 803/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0358 - acc: 0.9870\n",
      "Epoch 804/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0340 - acc: 0.9880\n",
      "Epoch 805/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0336 - acc: 0.9885\n",
      "Epoch 806/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0355 - acc: 0.9876\n",
      "Epoch 807/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0375 - acc: 0.9870\n",
      "Epoch 808/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0346 - acc: 0.9880\n",
      "Epoch 809/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0357 - acc: 0.9876\n",
      "Epoch 810/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0363 - acc: 0.9875\n",
      "Epoch 811/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0344 - acc: 0.9880\n",
      "Epoch 812/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0323 - acc: 0.9887\n",
      "Epoch 813/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0332 - acc: 0.9886\n",
      "Epoch 814/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0368 - acc: 0.9869\n",
      "Epoch 815/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0327 - acc: 0.9883\n",
      "Epoch 816/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0359 - acc: 0.9875\n",
      "Epoch 817/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0332 - acc: 0.9884\n",
      "Epoch 818/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0343 - acc: 0.9879\n",
      "Epoch 819/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0358 - acc: 0.9874\n",
      "Epoch 820/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0341 - acc: 0.9885\n",
      "Epoch 821/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0362 - acc: 0.9876: \n",
      "Epoch 822/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0352 - acc: 0.9880\n",
      "Epoch 823/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0355 - acc: 0.9879\n",
      "Epoch 824/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0336 - acc: 0.9887\n",
      "Epoch 825/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0365 - acc: 0.9875: \n",
      "Epoch 826/2000\n",
      "63873/63873 [==============================] - 6s 93us/step - loss: 0.0398 - acc: 0.9863\n",
      "Epoch 827/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0339 - acc: 0.9884\n",
      "Epoch 828/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0335 - acc: 0.9884\n",
      "Epoch 829/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0345 - acc: 0.9884\n",
      "Epoch 830/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0362 - acc: 0.9870\n",
      "Epoch 831/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0408 - acc: 0.9863\n",
      "Epoch 832/2000\n",
      "63873/63873 [==============================] - 1995s 31ms/step - loss: 0.0348 - acc: 0.9878\n",
      "Epoch 833/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0348 - acc: 0.9872\n",
      "Epoch 834/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0367 - acc: 0.9872\n",
      "Epoch 835/2000\n",
      "63873/63873 [==============================] - 12s 184us/step - loss: 0.0356 - acc: 0.9877\n",
      "Epoch 836/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0335 - acc: 0.9885\n",
      "Epoch 837/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0354 - acc: 0.9880\n",
      "Epoch 838/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0333 - acc: 0.9883\n",
      "Epoch 839/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0333 - acc: 0.9885: 1s - loss: \n",
      "Epoch 840/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0324 - acc: 0.9888\n",
      "Epoch 841/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0357 - acc: 0.9872\n",
      "Epoch 842/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0331 - acc: 0.9887\n",
      "Epoch 843/2000\n",
      "63873/63873 [==============================] - 2087s 33ms/step - loss: 0.0339 - acc: 0.9882\n",
      "Epoch 844/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0323 - acc: 0.9890: 0s - loss: 0\n",
      "Epoch 845/2000\n",
      "63873/63873 [==============================] - 9s 146us/step - loss: 0.0366 - acc: 0.9877\n",
      "Epoch 846/2000\n",
      "63873/63873 [==============================] - 7s 112us/step - loss: 0.0353 - acc: 0.9876\n",
      "Epoch 847/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0355 - acc: 0.9882\n",
      "Epoch 848/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0323 - acc: 0.9889\n",
      "Epoch 849/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0322 - acc: 0.9885\n",
      "Epoch 850/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0343 - acc: 0.9888\n",
      "Epoch 851/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0353 - acc: 0.9880\n",
      "Epoch 852/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0319 - acc: 0.9891\n",
      "Epoch 853/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0326 - acc: 0.9884\n",
      "Epoch 854/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0316 - acc: 0.9897\n",
      "Epoch 855/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0389 - acc: 0.9863\n",
      "Epoch 856/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0355 - acc: 0.9879: 0s - loss: 0.0361 - a\n",
      "Epoch 857/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0308 - acc: 0.9893\n",
      "Epoch 858/2000\n",
      "63873/63873 [==============================] - 5s 77us/step - loss: 0.0316 - acc: 0.9889\n",
      "Epoch 859/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0343 - acc: 0.9882\n",
      "Epoch 860/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0328 - acc: 0.9889\n",
      "Epoch 861/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0335 - acc: 0.9884\n",
      "Epoch 862/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0337 - acc: 0.9885\n",
      "Epoch 863/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0309 - acc: 0.9896\n",
      "Epoch 864/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0304 - acc: 0.9892\n",
      "Epoch 865/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 866/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0326 - acc: 0.9895\n",
      "Epoch 867/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0366 - acc: 0.9873\n",
      "Epoch 868/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0313 - acc: 0.9894\n",
      "Epoch 869/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0325 - acc: 0.9887\n",
      "Epoch 870/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0330 - acc: 0.9884\n",
      "Epoch 871/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0307 - acc: 0.9896\n",
      "Epoch 872/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0332 - acc: 0.9888\n",
      "Epoch 873/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0341 - acc: 0.9885\n",
      "Epoch 874/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0359 - acc: 0.9875\n",
      "Epoch 875/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0320 - acc: 0.9889\n",
      "Epoch 876/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0311 - acc: 0.9890\n",
      "Epoch 877/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0313 - acc: 0.9893\n",
      "Epoch 878/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0366 - acc: 0.9878\n",
      "Epoch 879/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0323 - acc: 0.9891\n",
      "Epoch 880/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0309 - acc: 0.9898: 0s - loss: 0.0299 - acc\n",
      "Epoch 881/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0325 - acc: 0.9885\n",
      "Epoch 882/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0340 - acc: 0.9887\n",
      "Epoch 883/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0318 - acc: 0.9889\n",
      "Epoch 884/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0314 - acc: 0.9896\n",
      "Epoch 885/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0339 - acc: 0.9887\n",
      "Epoch 886/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0337 - acc: 0.9888\n",
      "Epoch 887/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0318 - acc: 0.9894\n",
      "Epoch 888/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0322 - acc: 0.9890\n",
      "Epoch 889/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0352 - acc: 0.9877\n",
      "Epoch 890/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0308 - acc: 0.9892\n",
      "Epoch 891/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0328 - acc: 0.9887\n",
      "Epoch 892/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0305 - acc: 0.9895\n",
      "Epoch 893/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0332 - acc: 0.9884\n",
      "Epoch 894/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0306 - acc: 0.9895\n",
      "Epoch 895/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0324 - acc: 0.9887\n",
      "Epoch 896/2000\n",
      "63873/63873 [==============================] - 3s 48us/step - loss: 0.0297 - acc: 0.9897: 2s - loss: 0.0 - ETA: 1s - los\n",
      "Epoch 897/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0326 - acc: 0.9888\n",
      "Epoch 898/2000\n",
      "63873/63873 [==============================] - 3s 49us/step - loss: 0.0333 - acc: 0.9882: 0s - loss: 0.0329 - a\n",
      "Epoch 899/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0306 - acc: 0.9892\n",
      "Epoch 900/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0362 - acc: 0.9876\n",
      "Epoch 901/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 902/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0291 - acc: 0.9901\n",
      "Epoch 903/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0320 - acc: 0.9884\n",
      "Epoch 904/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0295 - acc: 0.9897\n",
      "Epoch 905/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0314 - acc: 0.9897\n",
      "Epoch 906/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0327 - acc: 0.9888\n",
      "Epoch 907/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0352 - acc: 0.9878\n",
      "Epoch 908/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0346 - acc: 0.9883\n",
      "Epoch 909/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0338 - acc: 0.9888\n",
      "Epoch 910/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0290 - acc: 0.9903\n",
      "Epoch 911/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0314 - acc: 0.9892\n",
      "Epoch 912/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0300 - acc: 0.9901\n",
      "Epoch 913/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0321 - acc: 0.9891\n",
      "Epoch 914/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0297 - acc: 0.9898\n",
      "Epoch 915/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0327 - acc: 0.9892\n",
      "Epoch 916/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0323 - acc: 0.9896\n",
      "Epoch 917/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0305 - acc: 0.9899\n",
      "Epoch 918/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0319 - acc: 0.9893\n",
      "Epoch 919/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0304 - acc: 0.9899\n",
      "Epoch 920/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0304 - acc: 0.9895\n",
      "Epoch 921/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0309 - acc: 0.9895\n",
      "Epoch 922/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0337 - acc: 0.9888\n",
      "Epoch 923/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0349 - acc: 0.9882\n",
      "Epoch 924/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0306 - acc: 0.9893\n",
      "Epoch 925/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0293 - acc: 0.9904\n",
      "Epoch 926/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0315 - acc: 0.9889\n",
      "Epoch 927/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0315 - acc: 0.9896\n",
      "Epoch 928/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0339 - acc: 0.9890\n",
      "Epoch 929/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0318 - acc: 0.9889\n",
      "Epoch 930/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0315 - acc: 0.9895\n",
      "Epoch 931/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0310 - acc: 0.9899\n",
      "Epoch 932/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 933/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0320 - acc: 0.9890\n",
      "Epoch 934/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0340 - acc: 0.9886\n",
      "Epoch 935/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0304 - acc: 0.9903\n",
      "Epoch 936/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0301 - acc: 0.9898\n",
      "Epoch 937/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0318 - acc: 0.9894\n",
      "Epoch 938/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0303 - acc: 0.9898\n",
      "Epoch 939/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0274 - acc: 0.9908\n",
      "Epoch 940/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0311 - acc: 0.9894\n",
      "Epoch 941/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0294 - acc: 0.9907\n",
      "Epoch 942/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0292 - acc: 0.9899\n",
      "Epoch 943/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0302 - acc: 0.9897\n",
      "Epoch 944/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0329 - acc: 0.9896\n",
      "Epoch 945/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0270 - acc: 0.9909\n",
      "Epoch 946/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0321 - acc: 0.9896\n",
      "Epoch 947/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0298 - acc: 0.9898\n",
      "Epoch 948/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0297 - acc: 0.9902\n",
      "Epoch 949/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0310 - acc: 0.9900\n",
      "Epoch 950/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0274 - acc: 0.9907\n",
      "Epoch 951/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0292 - acc: 0.9909\n",
      "Epoch 952/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0318 - acc: 0.9897\n",
      "Epoch 953/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0351 - acc: 0.9884\n",
      "Epoch 954/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0327 - acc: 0.9890\n",
      "Epoch 955/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0277 - acc: 0.9907\n",
      "Epoch 956/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0304 - acc: 0.9900\n",
      "Epoch 957/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0315 - acc: 0.9895\n",
      "Epoch 958/2000\n",
      "63873/63873 [==============================] - 3s 48us/step - loss: 0.0328 - acc: 0.9892\n",
      "Epoch 959/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0316 - acc: 0.9895\n",
      "Epoch 960/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0294 - acc: 0.9900\n",
      "Epoch 961/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0306 - acc: 0.9899\n",
      "Epoch 962/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0314 - acc: 0.9896\n",
      "Epoch 963/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0296 - acc: 0.9903\n",
      "Epoch 964/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0301 - acc: 0.9904\n",
      "Epoch 965/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0305 - acc: 0.9903\n",
      "Epoch 966/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0313 - acc: 0.9897\n",
      "Epoch 967/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0307 - acc: 0.9899\n",
      "Epoch 968/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0289 - acc: 0.9901\n",
      "Epoch 969/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0306 - acc: 0.9895\n",
      "Epoch 970/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0299 - acc: 0.9902\n",
      "Epoch 971/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0306 - acc: 0.9899\n",
      "Epoch 972/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0285 - acc: 0.9902\n",
      "Epoch 973/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0312 - acc: 0.9893\n",
      "Epoch 974/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0320 - acc: 0.9892\n",
      "Epoch 975/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0291 - acc: 0.9900\n",
      "Epoch 976/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0277 - acc: 0.9912\n",
      "Epoch 977/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0276 - acc: 0.9909\n",
      "Epoch 978/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0276 - acc: 0.9912\n",
      "Epoch 979/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0315 - acc: 0.9892\n",
      "Epoch 980/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0302 - acc: 0.9896\n",
      "Epoch 981/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 982/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0322 - acc: 0.9891\n",
      "Epoch 983/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0287 - acc: 0.9909\n",
      "Epoch 984/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0314 - acc: 0.9894\n",
      "Epoch 985/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0287 - acc: 0.9906\n",
      "Epoch 986/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0265 - acc: 0.9913\n",
      "Epoch 987/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0317 - acc: 0.9893\n",
      "Epoch 988/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0301 - acc: 0.9899\n",
      "Epoch 989/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0269 - acc: 0.9910\n",
      "Epoch 990/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0301 - acc: 0.9902\n",
      "Epoch 991/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0292 - acc: 0.9903\n",
      "Epoch 992/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0280 - acc: 0.9904\n",
      "Epoch 993/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0309 - acc: 0.9899\n",
      "Epoch 994/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0305 - acc: 0.9898\n",
      "Epoch 995/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0288 - acc: 0.9905\n",
      "Epoch 996/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0325 - acc: 0.9894\n",
      "Epoch 997/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0290 - acc: 0.9907\n",
      "Epoch 998/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0283 - acc: 0.9907\n",
      "Epoch 999/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0277 - acc: 0.9906\n",
      "Epoch 1000/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0297 - acc: 0.9903\n",
      "Epoch 1001/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0325 - acc: 0.9898\n",
      "Epoch 1002/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0298 - acc: 0.9898\n",
      "Epoch 1003/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0278 - acc: 0.9912\n",
      "Epoch 1004/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 1005/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0280 - acc: 0.9909\n",
      "Epoch 1006/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0275 - acc: 0.9905\n",
      "Epoch 1007/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0288 - acc: 0.9909\n",
      "Epoch 1008/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0304 - acc: 0.9901\n",
      "Epoch 1009/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0273 - acc: 0.9912\n",
      "Epoch 1010/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0285 - acc: 0.9905\n",
      "Epoch 1011/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 1012/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0284 - acc: 0.9908\n",
      "Epoch 1013/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0305 - acc: 0.9896\n",
      "Epoch 1014/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0286 - acc: 0.9902\n",
      "Epoch 1015/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0296 - acc: 0.9905\n",
      "Epoch 1016/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0284 - acc: 0.9904\n",
      "Epoch 1017/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0283 - acc: 0.9907\n",
      "Epoch 1018/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 1019/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0269 - acc: 0.9913\n",
      "Epoch 1020/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0300 - acc: 0.9902\n",
      "Epoch 1021/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0277 - acc: 0.9908\n",
      "Epoch 1022/2000\n",
      "63873/63873 [==============================] - 3s 53us/step - loss: 0.0281 - acc: 0.9907\n",
      "Epoch 1023/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0282 - acc: 0.9904\n",
      "Epoch 1024/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 1025/2000\n",
      "63873/63873 [==============================] - 3s 48us/step - loss: 0.0297 - acc: 0.9906\n",
      "Epoch 1026/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0285 - acc: 0.9907\n",
      "Epoch 1027/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0287 - acc: 0.9905\n",
      "Epoch 1028/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0294 - acc: 0.9905\n",
      "Epoch 1029/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0282 - acc: 0.9910\n",
      "Epoch 1030/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0275 - acc: 0.9907\n",
      "Epoch 1031/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0305 - acc: 0.9898\n",
      "Epoch 1032/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0267 - acc: 0.9911\n",
      "Epoch 1033/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0287 - acc: 0.9905\n",
      "Epoch 1034/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0291 - acc: 0.9904: 0s - loss: 0.0289 - acc: 0.\n",
      "Epoch 1035/2000\n",
      "63873/63873 [==============================] - 3s 55us/step - loss: 0.0268 - acc: 0.9911\n",
      "Epoch 1036/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0281 - acc: 0.9910: 1s - l\n",
      "Epoch 1037/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0291 - acc: 0.9902\n",
      "Epoch 1038/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0275 - acc: 0.9910\n",
      "Epoch 1039/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0295 - acc: 0.9908\n",
      "Epoch 1040/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 1041/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0290 - acc: 0.9905\n",
      "Epoch 1042/2000\n",
      "63873/63873 [==============================] - 4s 55us/step - loss: 0.0297 - acc: 0.9901\n",
      "Epoch 1043/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0273 - acc: 0.9913\n",
      "Epoch 1044/2000\n",
      "63873/63873 [==============================] - 161s 3ms/step - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 1045/2000\n",
      "63873/63873 [==============================] - 6s 88us/step - loss: 0.0287 - acc: 0.9902: 0s - lo\n",
      "Epoch 1046/2000\n",
      "63873/63873 [==============================] - 5s 74us/step - loss: 0.0264 - acc: 0.9914\n",
      "Epoch 1047/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0290 - acc: 0.9907\n",
      "Epoch 1048/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0306 - acc: 0.9899\n",
      "Epoch 1049/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0283 - acc: 0.9905\n",
      "Epoch 1050/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0279 - acc: 0.9908\n",
      "Epoch 1051/2000\n",
      "63873/63873 [==============================] - 5s 80us/step - loss: 0.0264 - acc: 0.9914\n",
      "Epoch 1052/2000\n",
      "63873/63873 [==============================] - 8s 127us/step - loss: 0.0275 - acc: 0.9909\n",
      "Epoch 1053/2000\n",
      "63873/63873 [==============================] - 7s 106us/step - loss: 0.0279 - acc: 0.9904\n",
      "Epoch 1054/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0268 - acc: 0.9911\n",
      "Epoch 1055/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0276 - acc: 0.9908\n",
      "Epoch 1056/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0275 - acc: 0.9913\n",
      "Epoch 1057/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0298 - acc: 0.9904\n",
      "Epoch 1058/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0267 - acc: 0.9910\n",
      "Epoch 1059/2000\n",
      "63873/63873 [==============================] - 5s 71us/step - loss: 0.0270 - acc: 0.9914\n",
      "Epoch 1060/2000\n",
      "63873/63873 [==============================] - 5s 83us/step - loss: 0.0306 - acc: 0.9898\n",
      "Epoch 1061/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0286 - acc: 0.9908\n",
      "Epoch 1062/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0294 - acc: 0.9903\n",
      "Epoch 1063/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0271 - acc: 0.9908\n",
      "Epoch 1064/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0277 - acc: 0.9915\n",
      "Epoch 1065/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0282 - acc: 0.9909\n",
      "Epoch 1066/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0282 - acc: 0.9904\n",
      "Epoch 1067/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0269 - acc: 0.9918\n",
      "Epoch 1068/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0283 - acc: 0.9907\n",
      "Epoch 1069/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0299 - acc: 0.9904\n",
      "Epoch 1070/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 1071/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0286 - acc: 0.9908\n",
      "Epoch 1072/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0279 - acc: 0.9908\n",
      "Epoch 1073/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0265 - acc: 0.9915\n",
      "Epoch 1074/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 1075/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0278 - acc: 0.9911\n",
      "Epoch 1076/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0301 - acc: 0.9900\n",
      "Epoch 1077/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0279 - acc: 0.9908\n",
      "Epoch 1078/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0286 - acc: 0.9907\n",
      "Epoch 1079/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0278 - acc: 0.9913\n",
      "Epoch 1080/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0275 - acc: 0.9916\n",
      "Epoch 1081/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0263 - acc: 0.9916\n",
      "Epoch 1082/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0296 - acc: 0.9902\n",
      "Epoch 1083/2000\n",
      "63873/63873 [==============================] - 6s 90us/step - loss: 0.0239 - acc: 0.9923\n",
      "Epoch 1084/2000\n",
      "63873/63873 [==============================] - 5s 75us/step - loss: 0.0302 - acc: 0.9900\n",
      "Epoch 1085/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0273 - acc: 0.9912\n",
      "Epoch 1086/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0264 - acc: 0.9919\n",
      "Epoch 1087/2000\n",
      "63873/63873 [==============================] - 5s 73us/step - loss: 0.0275 - acc: 0.9908\n",
      "Epoch 1088/2000\n",
      "63873/63873 [==============================] - 6s 95us/step - loss: 0.0287 - acc: 0.9907\n",
      "Epoch 1089/2000\n",
      "63873/63873 [==============================] - 6s 95us/step - loss: 0.0281 - acc: 0.9910\n",
      "Epoch 1090/2000\n",
      "63873/63873 [==============================] - 6s 98us/step - loss: 0.0252 - acc: 0.9919\n",
      "Epoch 1091/2000\n",
      "63873/63873 [==============================] - 6s 92us/step - loss: 0.0299 - acc: 0.9904\n",
      "Epoch 1092/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0271 - acc: 0.9910\n",
      "Epoch 1093/2000\n",
      "63873/63873 [==============================] - 4s 70us/step - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 1094/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0269 - acc: 0.9914\n",
      "Epoch 1095/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0290 - acc: 0.9904: 0s - loss: 0.0291 - acc: 0.9\n",
      "Epoch 1096/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0276 - acc: 0.9911\n",
      "Epoch 1097/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0265 - acc: 0.9918\n",
      "Epoch 1098/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0274 - acc: 0.9914\n",
      "Epoch 1099/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0239 - acc: 0.9924\n",
      "Epoch 1100/2000\n",
      "63873/63873 [==============================] - 4s 56us/step - loss: 0.0277 - acc: 0.9908\n",
      "Epoch 1101/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0253 - acc: 0.9922\n",
      "Epoch 1102/2000\n",
      "63873/63873 [==============================] - 3s 52us/step - loss: 0.0267 - acc: 0.9909\n",
      "Epoch 1103/2000\n",
      "63873/63873 [==============================] - 4s 61us/step - loss: 0.0282 - acc: 0.9910\n",
      "Epoch 1104/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0237 - acc: 0.9924\n",
      "Epoch 1105/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0263 - acc: 0.9913\n",
      "Epoch 1106/2000\n",
      "63873/63873 [==============================] - 4s 62us/step - loss: 0.0290 - acc: 0.9905\n",
      "Epoch 1107/2000\n",
      "63873/63873 [==============================] - 4s 66us/step - loss: 0.0246 - acc: 0.9925\n",
      "Epoch 1108/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0294 - acc: 0.9904\n",
      "Epoch 1109/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 1110/2000\n",
      "63873/63873 [==============================] - 4s 69us/step - loss: 0.0263 - acc: 0.9917\n",
      "Epoch 1111/2000\n",
      "63873/63873 [==============================] - 4s 67us/step - loss: 0.0247 - acc: 0.9921\n",
      "Epoch 1112/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0275 - acc: 0.9912\n",
      "Epoch 1113/2000\n",
      "63873/63873 [==============================] - 4s 60us/step - loss: 0.0258 - acc: 0.9919\n",
      "Epoch 1114/2000\n",
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0249 - acc: 0.9919\n",
      "Epoch 1115/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0250 - acc: 0.9924\n",
      "Epoch 1116/2000\n",
      "63873/63873 [==============================] - 3s 50us/step - loss: 0.0257 - acc: 0.9916\n",
      "Epoch 1117/2000\n",
      "63873/63873 [==============================] - 3s 51us/step - loss: 0.0257 - acc: 0.9920\n",
      "Epoch 1118/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0276 - acc: 0.9909\n",
      "Epoch 1119/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0272 - acc: 0.9907\n",
      "Epoch 1120/2000\n",
      "63873/63873 [==============================] - 3s 54us/step - loss: 0.0251 - acc: 0.9923\n",
      "Epoch 1121/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0281 - acc: 0.9908\n",
      "Epoch 1122/2000\n",
      "63873/63873 [==============================] - 4s 59us/step - loss: 0.0272 - acc: 0.9912\n",
      "Epoch 1123/2000\n",
      "63873/63873 [==============================] - 4s 57us/step - loss: 0.0268 - acc: 0.9918\n",
      "Epoch 1124/2000\n",
      "63873/63873 [==============================] - 4s 58us/step - loss: 0.0261 - acc: 0.9919\n",
      "Epoch 1125/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0278 - acc: 0.9908\n",
      "Epoch 1126/2000\n",
      "63873/63873 [==============================] - 6s 86us/step - loss: 0.0288 - acc: 0.9907\n",
      "Epoch 1127/2000\n",
      "63873/63873 [==============================] - 5s 79us/step - loss: 0.0248 - acc: 0.9919\n",
      "Epoch 1128/2000\n",
      "63873/63873 [==============================] - 5s 78us/step - loss: 0.0267 - acc: 0.9912\n",
      "Epoch 1129/2000\n",
      "63873/63873 [==============================] - 4s 68us/step - loss: 0.0272 - acc: 0.9909\n",
      "Epoch 1130/2000\n",
      "63873/63873 [==============================] - 4s 65us/step - loss: 0.0269 - acc: 0.9915\n",
      "Epoch 1131/2000\n",
      "63873/63873 [==============================] - 4s 63us/step - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 1132/2000\n",
      "63873/63873 [==============================] - 4s 64us/step - loss: 0.0275 - acc: 0.9914\n",
      "Epoch 1133/2000\n",
      "63873/63873 [==============================] - 5s 84us/step - loss: 0.0427 - acc: 0.9854\n",
      "Epoch 1134/2000\n",
      "31360/63873 [=============>................] - ETA: 3s - loss: 0.0253 - acc: 0.9914"
     ]
    }
   ],
   "source": [
    "model_dnn.fit(X_train, Y_train, epochs=2000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212911/212911 [==============================] - 12s 55us/step\n",
      "Loss = 0.227156401941\n",
      "Test Accuracy = 0.886412632508\n"
     ]
    }
   ],
   "source": [
    "# Print Test Accuracy: \n",
    "preds = model_dnn.evaluate(x = X_test, y = Y_test)\n",
    "### END CODE HERE ###\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dnn.predict(X_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638730/638730 [==============================] - 30s 47us/step\n",
      "Loss = 0.222039635102\n",
      "Train Accuracy = 0.890124152615\n"
     ]
    }
   ],
   "source": [
    "preds = model_dnn.evaluate(x = x_train, y = y_train)\n",
    "### END CODE HERE ###\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Train Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Defind a creat_model() function that returns a model according to different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyper_parameters_1 = {\"epochs\": [1,2,3], \"batch_size\" : [4,8,16,32,64,128], \\n                    \\'learn_rate\\': [0.001, 0.005, 0.01, 0.02, 0.1],\\n                    \\'init_mode\\': [\\'uniform\\', \\'lecun_uniform\\', \\'normal\\', \\'zero\\'],\\n                    \\'dropout_rate\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \\n                    \\'neurons\\': [1, 5, 10, 15, 20, 25, 30],\\n                     \\'decay\\':[0.0001,0.001,0.01,0.1]}'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''hyper_parameters_1 = {\"epochs\": [1,2,3], \"batch_size\" : [4,8,16,32,64,128], \n",
    "                    'learn_rate': [0.001, 0.005, 0.01, 0.02, 0.1],\n",
    "                    'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero'],\n",
    "                    'dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "                    'neurons': [1, 5, 10, 15, 20, 25, 30],\n",
    "                     'decay':[0.0001,0.001,0.01,0.1]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters_1 = {\"epochs\": [10], \"batch_size\" : [8,32,64,128],\n",
    "                    'learn_rate': [0.00001, 0.0001,0.001,0.01],\n",
    "                    'init_mode':  ['uniform', 'normal', 'zero'],\n",
    "                    'dropout_rate': [0.0, 0.2, 0.4, 0.6 ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#model_dnn.reset_states()\\nmodel_dnn = Sequential()\\ndim = X_train.shape[1]\\nmodel_dnn.add(Dense(64, activation='relu', input_dim=dim, kernel_initializer='uniform'))\\n# model_dnn.add(Dropout(0.2))\\nmodel_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\\n#model_dnn.add(Dropout(0.2))\\nmodel_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\\n# model_dnn.add(Dropout(0.2))\\nmodel_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\\n# model_dnn.add(Dropout(0.2))\\nmodel_dnn.add(Dense(32, activation='relu', kernel_initializer='uniform'))\\n# model_dnn.add(Dropout(0.2))\\nmodel_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\\nmodel_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\\n# model_dnn.add(Dropout(0.2))\\n# model.add(Dense(1, activation='relu'))\\n# model.add(Flatten())\\nmodel_dnn.add(Dense(2, activation='softmax'))\\n# adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)\\n# model.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\\nmodel_dnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(dropout_rate=0.0, init_mode = 'uniform', learn_rate = 0.001):\n",
    "    #model_dnn.reset_states()\n",
    "    model_dnn = Sequential()\n",
    "    dim = X_train.shape[1]\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu',   input_dim=dim))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(32, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(16, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(16, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    # model.add(Dense(1, activation='relu'))\n",
    "    # model.add(Flatten())\n",
    "    model_dnn.add(Dense(2, activation='softmax'))\n",
    "    # adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "    # model.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    optimizer = keras.optimizers.Adam(lr=learn_rate)\n",
    "    model_dnn.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # model_dnn.fit(X_train, y_train, epochs=100, batch_size=64)\n",
    "    return model_dnn\n",
    "\n",
    "'''#model_dnn.reset_states()\n",
    "model_dnn = Sequential()\n",
    "dim = X_train.shape[1]\n",
    "model_dnn.add(Dense(64, activation='relu', input_dim=dim, kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "#model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(64, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(32, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "model_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\n",
    "model_dnn.add(Dense(16, activation='relu', kernel_initializer='uniform'))\n",
    "# model_dnn.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='relu'))\n",
    "# model.add(Flatten())\n",
    "model_dnn.add(Dense(2, activation='softmax'))\n",
    "# adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00001, amsgrad=False)\n",
    "# model.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_para_tuning:  (1277, 57)\n",
      "shape of Y_para_tuning: (1277,)\n"
     ]
    }
   ],
   "source": [
    "# sample a small portion of my dataset to accelrate thie process.\n",
    "np.random.seed(1)\n",
    "rand_numbers = np.random.randint(0, len(X_train), int(len(X_train)*0.002))\n",
    "X_para_tuning = X_train[rand_numbers]\n",
    "Y_para_tuning = Y_train[rand_numbers]\n",
    "\n",
    "print(\"shape of X_para_tuning: \", X_para_tuning.shape)\n",
    "print(\"shape of Y_para_tuning:\", Y_para_tuning.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose = 1)\n",
    "# model.fit(X_para_tuning,Y_para_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_parameters_1 = {\"batch_size\": [10, 20, 40],\n",
    "# \"epochs\" [1,2,3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] epochs=10 .......................................................\n",
      "Epoch 1/10\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.6603 - acc: 0.8120\n",
      "Epoch 2/10\n",
      "851/851 [==============================] - 0s 183us/step - loss: 0.4857 - acc: 0.8132\n",
      "Epoch 3/10\n",
      "851/851 [==============================] - 0s 134us/step - loss: 0.4386 - acc: 0.8132\n",
      "Epoch 4/10\n",
      "851/851 [==============================] - 0s 137us/step - loss: 0.4047 - acc: 0.8132\n",
      "Epoch 5/10\n",
      "851/851 [==============================] - 0s 144us/step - loss: 0.3500 - acc: 0.8132\n",
      "Epoch 6/10\n",
      "851/851 [==============================] - 0s 140us/step - loss: 0.2941 - acc: 0.8132\n",
      "Epoch 7/10\n",
      "851/851 [==============================] - 0s 162us/step - loss: 0.2650 - acc: 0.8132\n",
      "Epoch 8/10\n",
      "851/851 [==============================] - 0s 145us/step - loss: 0.2441 - acc: 0.8343\n",
      "Epoch 9/10\n",
      "851/851 [==============================] - 0s 157us/step - loss: 0.2263 - acc: 0.8989\n",
      "Epoch 10/10\n",
      "851/851 [==============================] - 0s 130us/step - loss: 0.2070 - acc: 0.9154\n",
      "426/426 [==============================] - 2s 4ms/step\n",
      "851/851 [==============================] - 0s 138us/step\n",
      "[CV] ........................................ epochs=10, total=   7.9s\n",
      "[CV] epochs=10 .......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.6710 - acc: 0.7850\n",
      "Epoch 2/10\n",
      "851/851 [==============================] - 0s 153us/step - loss: 0.5190 - acc: 0.8049\n",
      "Epoch 3/10\n",
      "851/851 [==============================] - 0s 154us/step - loss: 0.4601 - acc: 0.8049\n",
      "Epoch 4/10\n",
      "851/851 [==============================] - 0s 129us/step - loss: 0.4269 - acc: 0.8049\n",
      "Epoch 5/10\n",
      "851/851 [==============================] - 0s 140us/step - loss: 0.3722 - acc: 0.8049\n",
      "Epoch 6/10\n",
      "851/851 [==============================] - 0s 128us/step - loss: 0.3399 - acc: 0.8049\n",
      "Epoch 7/10\n",
      "851/851 [==============================] - 0s 145us/step - loss: 0.2975 - acc: 0.8049\n",
      "Epoch 8/10\n",
      "851/851 [==============================] - 0s 134us/step - loss: 0.2824 - acc: 0.8049\n",
      "Epoch 9/10\n",
      "851/851 [==============================] - 0s 143us/step - loss: 0.2771 - acc: 0.8049\n",
      "Epoch 10/10\n",
      "851/851 [==============================] - 0s 130us/step - loss: 0.2603 - acc: 0.8049\n",
      "426/426 [==============================] - 2s 4ms/step\n",
      "851/851 [==============================] - 0s 133us/step\n",
      "[CV] ........................................ epochs=10, total=   7.6s\n",
      "[CV] epochs=10 .......................................................\n",
      "Epoch 1/10\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.6761 - acc: 0.7688\n",
      "Epoch 2/10\n",
      "852/852 [==============================] - 0s 173us/step - loss: 0.5641 - acc: 0.7805\n",
      "Epoch 3/10\n",
      "852/852 [==============================] - 0s 141us/step - loss: 0.4874 - acc: 0.7805\n",
      "Epoch 4/10\n",
      "852/852 [==============================] - 0s 147us/step - loss: 0.4554 - acc: 0.7805\n",
      "Epoch 5/10\n",
      "852/852 [==============================] - 0s 133us/step - loss: 0.4093 - acc: 0.7805\n",
      "Epoch 6/10\n",
      "852/852 [==============================] - 0s 153us/step - loss: 0.3576 - acc: 0.7805\n",
      "Epoch 7/10\n",
      "852/852 [==============================] - 0s 134us/step - loss: 0.3148 - acc: 0.7805\n",
      "Epoch 8/10\n",
      "852/852 [==============================] - 0s 151us/step - loss: 0.2867 - acc: 0.7805\n",
      "Epoch 9/10\n",
      "852/852 [==============================] - 0s 128us/step - loss: 0.2683 - acc: 0.8075\n",
      "Epoch 10/10\n",
      "852/852 [==============================] - 0s 154us/step - loss: 0.2816 - acc: 0.8650\n",
      "425/425 [==============================] - 2s 4ms/step\n",
      "852/852 [==============================] - 0s 91us/step\n",
      "[CV] ........................................ epochs=10, total=   7.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   23.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1277/1277 [==============================] - 3s 3ms/step - loss: 0.6392 - acc: 0.7980\n",
      "Epoch 2/10\n",
      "1277/1277 [==============================] - 0s 129us/step - loss: 0.4788 - acc: 0.7995\n",
      "Epoch 3/10\n",
      "1277/1277 [==============================] - 0s 161us/step - loss: 0.4304 - acc: 0.7995\n",
      "Epoch 4/10\n",
      "1277/1277 [==============================] - 0s 142us/step - loss: 0.3678 - acc: 0.7995\n",
      "Epoch 5/10\n",
      "1277/1277 [==============================] - 0s 154us/step - loss: 0.3147 - acc: 0.7995\n",
      "Epoch 6/10\n",
      "1277/1277 [==============================] - 0s 143us/step - loss: 0.2846 - acc: 0.8121\n",
      "Epoch 7/10\n",
      "1277/1277 [==============================] - 0s 137us/step - loss: 0.2694 - acc: 0.8841\n",
      "Epoch 8/10\n",
      "1277/1277 [==============================] - 0s 148us/step - loss: 0.2544 - acc: 0.8849\n",
      "Epoch 9/10\n",
      "1277/1277 [==============================] - 0s 146us/step - loss: 0.2312 - acc: 0.8966\n",
      "Epoch 10/10\n",
      "1277/1277 [==============================] - 0s 135us/step - loss: 0.2297 - acc: 0.8982\n"
     ]
    }
   ],
   "source": [
    "param_grid = hyper_parameters_1\n",
    "# GridSearchCV use the default 3-fold cross validation,\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2, n_jobs=1)\n",
    "grid_result = grid.fit(X_para_tuning, Y_para_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use best epoches and batch size to tune optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.818324 using {'epochs': 10}\n",
      "0.818324 (0.021757) with: {'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
