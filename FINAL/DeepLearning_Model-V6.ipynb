{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@Project: Connexin Group \\n\\n@FileName: DeepLearning_Model_V3\\nf\\n@Author：Zhejian Peng\\n\\n@Create date: Mar. 25th, 2018\\n\\n@description：reduce dimension of our dataset using pca, without normalizing categorical data.\\n\\n@Update date：Mar. 25th, 2018\\n            Try to split train and test before normalization.\\n            1. Need to update normalization for zipcode on V3\\n            2. Update drop2 to drop more features that might leak information\\n            \\n            Update_deep learing model-V and make comparison with logistic Regression\\n            \\n            April. 6th, 2018 V3\\n            1. Deploy Model for validation\\n            2. Visualize our Deep Learning Model\\n            \\n            April. 21, 2018 V5\\n            1. Use finalized datasets.\\n            2. Try Overfit the model and finalize the model\\n            \\n            April. 22, 2018 V5\\n            1. Use tuned paramters for final model\\n@Vindicator：  \\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@Project: Connexin Group \n",
    "\n",
    "@FileName: DeepLearning_Model_V3\n",
    "f\n",
    "@Author：Zhejian Peng\n",
    "\n",
    "@Create date: Mar. 25th, 2018\n",
    "\n",
    "@description：reduce dimension of our dataset using pca, without normalizing categorical data.\n",
    "\n",
    "@Update date：Mar. 25th, 2018\n",
    "            Try to split train and test before normalization.\n",
    "            1. Need to update normalization for zipcode on V3\n",
    "            2. Update drop2 to drop more features that might leak information\n",
    "            \n",
    "            Update_deep learing model-V and make comparison with logistic Regression\n",
    "            \n",
    "            April. 6th, 2018 V3\n",
    "            1. Deploy Model for validation\n",
    "            2. Visualize our Deep Learning Model\n",
    "            \n",
    "            April. 21, 2018 V5\n",
    "            1. Use finalized datasets.\n",
    "            2. Try Overfit the model and finalize the model\n",
    "            \n",
    "            April. 22, 2018 V5\n",
    "            1. Use tuned paramters for final model\n",
    "@Vindicator：  \n",
    "\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Select all categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split , cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in our data Frame\n",
    "def readcsv(file_path):\n",
    "    LARGE_FILE = file_path\n",
    "    CHUNKSIZE = 100000 # processing 100,000 rows at a time\n",
    "    # Add encoding encoding = \"ISO-8859-1\", why?\n",
    "    reader = pd.read_csv(LARGE_FILE, chunksize=CHUNKSIZE, low_memory=False, encoding = \"ISO-8859-1\")\n",
    "    frames = []\n",
    "    for df in reader:\n",
    "        frames.append(df)\n",
    "    loan_data = pd.concat(frames)\n",
    "    return loan_data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"/Users/zhejianpeng/Google Drive File Stream/My Drive/MSFE-UIUC/MSFE-TWO/Practicum/Week7/loan_data_no_current_converted.csv\"\n",
    "loan_data = readcsv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = loan_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert Verification_status_joint, add this categorical data to the categorical list\n",
    "for idx, i in df[\"verification_status_joint\"].iteritems():\n",
    "    if i == \"Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 1\n",
    "    elif i == \"Source Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 2\n",
    "    elif i == \"Not Verified\":\n",
    "        df.at[idx, \"verification_status_joint\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical = ['grade', 'sub_grade', 'emp_length', 'purpose', 'title', 'application_type', 'hardship_flag', 'hardship_type', 'hardship_reason', \n",
    "              'hardship_status', 'hardship_loan_status', 'settlement_status', 'disbursement_method', 'home_ownership',\n",
    "              'pymnt_plan', 'debt_settlement_flag', 'title', 'initial_list_status', 'loan_status', 'verification_status',\n",
    "              'term', 'verification_status_joint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 categorical data in our dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d categorical data in our dataset.\" % len(categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in engineered features.\n",
    "FE_PATH = \"./This_week_FE.csv\"\n",
    "df_FE = readcsv(FE_PATH)\n",
    "# temp.replace(float('nan'), -9999999, inplace =True)\n",
    "# temp = df_FE[df_FE['ratio_rev_acct']!='#DIV/0!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FE['ratio_rev_acct'].replace('#DIV/0!', float(-np.inf), inplace = True)\n",
    "df_FE.loc[:,'ratio_rev_acct'] = [float(x) for x in df_FE['ratio_rev_acct']]\n",
    "\n",
    "df_FE['ratio_rev_acct'].replace(float(-np.inf), np.max(df_FE['ratio_rev_acct']), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_FE['loan_amt_to_avg_inc'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season\n",
      "loan_amt_to_avg_inc\n",
      "coll_to_cur\n"
     ]
    }
   ],
   "source": [
    "# Replace '#DIV/0!' with the max of each col\n",
    "for i in df_FE.columns[0:3]:\n",
    "    df_FE[i].replace('#DIV/0!', np.max(df_FE[i]), inplace = True)\n",
    "    df_FE.loc[:,i] = [float(x) for x in df_FE[i]]\n",
    "\n",
    "    print(i)\n",
    "\n",
    "df = pd.concat([df,df_FE],axis=1)\n",
    "assert(df_FE.shape[0] == df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我写了一堆code 然后发现其实简单一点就能弄出来，所以大家忽略后面的code！！！\n",
    "# I have wrote a lot of code for this only to find out that I only need this simple function!!!\n",
    "def norm_inc_by_zip(zipcode, income):\n",
    "    '''\n",
    "        @description: Use on a column of data; output a dictionary that returns mean and average in each zipcode area\n",
    "        @zipcode： zipcode dataframe column\n",
    "        @income: income df column \n",
    "        @return:      return a dictionary\n",
    "    '''  \n",
    "    # I try to replace nan with 0 for income, and nan in zipcode for \"000xx\"\n",
    "    df[\"annual_inc\"].fillna(0)\n",
    "    df[\"zip_code\"].fillna(\"000xx\")\n",
    "    \n",
    "    mean_var = {}\n",
    "    for idx, value in zipcode.iteritems():\n",
    "        # calculate total income\n",
    "        if value in mean_var:\n",
    "            mean_var[value].append(income[idx])\n",
    "        else:\n",
    "            mean_var[value] = [income[idx]]\n",
    "\n",
    "    \n",
    "    #assert(len(zip_code) == len(mean_var))\n",
    "    # compute the average income in each zip_code area\n",
    "    for key, value in mean_var.items():\n",
    "        # if there only one element, we set their variance to 1. This way when normalize, it will have a 0 z-score.\n",
    "        if len(value) == 1:\n",
    "            #print(value[0])\n",
    "            mean_var[key] = [value[0], 1]\n",
    "        else:\n",
    "            mean_var[key] = [np.mean(value), np.std(value)]\n",
    "        \n",
    "    # first loop through every annual income by calculate its z score. (Income - mean_by_zipcode) / variance_by_zipcode\n",
    "    for idx, value in df[\"zip_code\"].iteritems():\n",
    "        #inc_colnum = df.columns.get_loc(\"annual_inc\")\n",
    "        col_num_inc = df[\"annual_inc\"]\n",
    "        mean, std = mean_var[value]\n",
    "        df.at[idx, \"annual_inc\"] = (df.at[idx, \"annual_inc\"] - mean) / std\n",
    "    print(\"Income is successfually normalized\")\n",
    "    return mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income is successfually normalized\n"
     ]
    }
   ],
   "source": [
    "dic = norm_inc_by_zip(df[\"zip_code\"], df[\"annual_inc\"])\n",
    "df.drop('zip_code', axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Set, X, Y, Train/Test Sets And normalize it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(x):\n",
    "    x = np.str(x)\n",
    "    if x[-1] == '%':\n",
    "        x = x[0:len(x)-1]\n",
    "    return float(x) / 100\n",
    "\n",
    "\n",
    "df['revol_util'] = [percentage(x) for x in df['revol_util']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dataset size is :  (851641, 58)\n"
     ]
    }
   ],
   "source": [
    "# drop the observation that was missing for any field\n",
    "df.dropna(axis=0, how='all', inplace=True)\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "# Use Finalized droplist provide by Yufei Gao\n",
    "drop_list=['installment','term','settlement_date','pymnt_plan','hardship_length', 'settlement_percentage', 'settlement_term', 'sec_app_earliest_cr_line','policy_code','hardship_end_date','settlement_amount',\n",
    "           'payment_plan_start_date','hardship_start_date','out_prncp','emp_title','title','earliest_cr_line','desc','issue_d','id','member_id','url','grade','sub_grade',\n",
    "                   'int_rate','avg_cur_bal','out_prncp_inv','debt_settlement_flag_date','hardship_amount','hardship_reason','addr_state','funded_amnt','funded_amnt_inv','collection_recovery_fee',\n",
    "                   'collections_12_mths_ex_med','mths_since_last_major_derog','next_pymnt_d','recoveries','total_pymnt',\n",
    "                   'total_pymnt_inv','total_rec_int','last_pymnt_d','last_credit_pull_d',\n",
    "                  'total_rec_prncp','settlement_status','hardship_loan_status','hardship_status','debt_settlement_flag',\n",
    "                   'verification_status','total_rec_late_fee','verification_status_joint','hardship_flag', 'hardship_type', 'hardship_reason'\n",
    "                    'hardship_status','hardship_loan_status','acc_now_delinq','delinq_amnt','deferral_term','hardship_amount'\n",
    "                    'hardship_length','hardship_dpd','hardship_payoff_balance_amount','hardship_last_payment_amount']\n",
    "\n",
    "\n",
    "# Drop drop_updated\n",
    "df.drop(drop_list, inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "# Drop all colums where value missed more than 20%\n",
    "num_rows=df.count(axis=0)\n",
    "df=df.iloc[:,(num_rows>=0.8*len(df)).tolist()]\n",
    "\n",
    "# Then fill rest of missing value with mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "# Drop all rows with 4,5,6\n",
    "'''for idx, i in loan_data[\"loan_status\"].iteritems():\n",
    "    if i == \"Fully Paid\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 1\n",
    "    elif i == \"Does not meet the credit policy. Status:Fully Paid\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 2\n",
    "    elif i == \"Does not meet the credit policy. Status:Charged Off\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 3\n",
    "    elif i == \"In Grace Period\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 4\n",
    "    elif i == \"Late (16-30 days)\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 5\n",
    "    elif i == \"Late (31-120 days)\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 6\n",
    "    elif i == \"Default\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 7\n",
    "    elif i == \"Charged Off\":\n",
    "        loan_data.at[idx, \"loan_status\"] = 8'''\n",
    "\n",
    "\n",
    "# Let's test our result previous result, Previous result set Y to binary number 1,2.\n",
    "# 4,5,6 are dropped, and we dont care about ‘Credit policy'\n",
    "df = df[(df['loan_status']!=4) & (df['loan_status']!=5) & (df['loan_status']!=6)]\n",
    "print(\"Input Dataset size is : \",df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['revol_util']:\n",
    "    if math.isnan(i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select features that is not in categorical data to normalize: categorial[], verification_status_joint, and annual_inc\n",
    "# There categorical features are features need to include in X\n",
    "features= list(df.columns)\n",
    "features_need_norm = []\n",
    "categorical_features = []\n",
    "for i in features:\n",
    "    if i not in categorical and i != \"verification_status_joint\":\n",
    "        features_need_norm.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 51 numerical features need normalization\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d numerical features need normalization\" %len(features_need_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = df.loc[:,['loan_status']].values\n",
    "features.remove(\"loan_status\")\n",
    "categorical_features.remove('loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can not have loan_status in X, we want to check this, if it prints \"Warning\", we have a problem!!!\n",
    "for i in categorical_features:\n",
    "    if i == \"loan_status\":\n",
    "        print(\"Warning!\")\n",
    "for i in features_need_norm:\n",
    "    if i == \"loan_status\":\n",
    "        print(\"Warning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our result previous result, Previous result set Y to binary number 1,2.\n",
    "\n",
    "Y = list(Y.reshape(len(Y)))\n",
    "for i in range(len(Y)):\n",
    "    if Y[i]==7 or Y[i]==8 or Y[i] == 3:\n",
    "        Y[i] = 1 # Default\n",
    "    else:\n",
    "        Y[i] = 0 # Fully Paid\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in Y:\n",
    "    if i != 0 and i!=1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I want to find the starting index and ending index of categorical data in X. \n",
    "# starting_col_index_of_categorical_data: starting index of categorical data in X\n",
    "# last_col_index_in_X: ending index of categorical data in X\n",
    "X = df.loc[:,features_need_norm].values\n",
    "starting_col_index_of_categorical_data = X.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.loc[:,categorical_features].values.shape = (891823, 16)\n",
    "X = np.concatenate((X, df.loc[:,categorical_features].values), axis=1)\n",
    "last_col_index_in_X = X.shape[1]-1\n",
    "# X.shape = (891823, 109)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical featurs in dataset: 6\n",
      "Number of numerical features in dataset: 51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of categorical featurs in dataset:\", len(categorical_features))\n",
    "print(\"Number of numerical features in dataset:\", len(features_need_norm))\n",
    "# From column index 93 to column index 107 are 15 categorical data in X\n",
    "# len(categorical_features) = 15\n",
    "last_col_index_in_X-starting_col_index_of_categorical_data+1 == len(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split train test set \n",
    "Y = np.ravel(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=1, test_size=0.25)\n",
    "# x_train, x_test, y_train, y_test = log_reg.split(X,Y,rand=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (638730, 57)\n",
      "shape of x_test:  (212911, 57)\n",
      "shape of y_train: (638730,)\n",
      "shape of y_test: (212911,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Normalize Train set and set mean variance in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize(numerical_features):\n",
    "    '''\n",
    "        @description: normalize all numerical features in X, and return a dictionary with mean variance of each features\n",
    "        @numerical_features： a numpy array of numerical features of shape (668867, 79)\n",
    "        @return: a dictionary contain mean variance of each features\n",
    "    '''  \n",
    "        # Step1: calculate mean variance of each columns in numerical features\n",
    "    epsilon = 10**-8\n",
    "    dic = {}\n",
    "    counter = 0\n",
    "    for columns in numerical_features.T:\n",
    "        mean  = np.mean(columns)\n",
    "        std = np.std(columns)\n",
    "        dic[counter] = [mean, std]\n",
    "        counter +=1\n",
    "\n",
    "    assert(counter == numerical_features.shape[1])\n",
    "\n",
    "    # Step2: Normalize numerical_features\n",
    "    for key, val in dic.items():\n",
    "        try :\n",
    "        #numerical_features[:,key] = (numerical_features[:,key] - val[0]) / val[1]\n",
    "            numerical_features[:, key] = (numerical_features[:,key] - val[0]) / val[1]\n",
    "            assert(np.mean(numerical_features[:, key])- 0.0 < epsilon)\n",
    "            assert(np.std(numerical_features[:, key])- 1.0 < epsilon)\n",
    "        except AssertionError:\n",
    "            print(np.mean(numerical_features[:, key]),\" | \", np.std(numerical_features[:, key]))\n",
    "            print(numerical_features[:,key], \"Key: \", key)\n",
    "            print(\"SUM: \", np.sum(numerical_features[:, key]))\n",
    "    return dic\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [3.5, 2.5], 1: [4.5, 2.5], 2: [5.5, 2.5], 3: [6.5, 2.5], 4: [7.5, 2.5]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "normalize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mv_dic = normalize(x_train[:,0: starting_col_index_of_categorical_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize test set according to train set's mean variance\n",
    "for key,val in mv_dic.items():\n",
    "    x_test[:, key] = (x_test[:,key] - val[0] ) / val[1]\n",
    "    # print(np.mean(x_test[:, key]))\n",
    "    # print(np.std(x_test[:, key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ratio \n",
      " 0.20687047639624068  |  0.7931295236037593\n",
      "Train set ratio \n",
      " 0.2065348425782412  |  0.7934651574217588\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in y_test:\n",
    "    if i==1:\n",
    "        counter +=1\n",
    "    \n",
    "print('Train set ratio \\n', counter/len(y_test), \" | \", (len(y_test)-counter) / len(y_test))\n",
    "    \n",
    "counter = 0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        counter +=1\n",
    "    \n",
    "print('Train set ratio \\n', counter/len(y_train), \" | \", (len(y_train)-counter) / len(y_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.791047\n",
       "8    0.205585\n",
       "2    0.002334\n",
       "3    0.000894\n",
       "7    0.000140\n",
       "Name: loan_status, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loan_status\"].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Train a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with tuned paramters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train:  (638730, 57)\n",
      "shape of X_test:  (212911, 57)\n",
      "shape of Y_train: (638730,)\n",
      "shape of Y_test: (212911,)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "# Use small data set to test for overfitting\n",
    "np.random.seed(1)\n",
    "# rand_numbers = np.random.randint(0, len(x_train), int(len(x_train)*0.1))\n",
    "# X_train = x_train[rand_numbers]\n",
    "# X_test = x_test\n",
    "# Y_train = y_train[rand_numbers]\n",
    "# Y_test = y_test\n",
    "\n",
    "X_train = x_train\n",
    "X_test = x_test\n",
    "\n",
    "Y_train = y_train\n",
    "Y_test = y_test\n",
    "\n",
    "print(\"shape of X_train: \", X_train.shape)\n",
    "print(\"shape of X_test: \", X_test.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Keras to construct a sequential model and visualize it \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "import keras\n",
    "import keras.backend as tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best: 0.867080 using {'batch_size': 8, 'dropout_rate': 0.0, \n",
    "# 'epochs': 10, 'init_mode': 'normal', 'learn_rate': 0.001}\n",
    "\n",
    "def create_model(dropout_rate=0.0, init_mode = 'normal', learn_rate = 0.001):\n",
    "    #model_dnn.reset_states()\n",
    "    model_dnn = Sequential()\n",
    "    dim = X_train.shape[1]\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu',   input_dim=dim))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "\n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(64, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(32, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(16, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model_dnn.add(Dense(16, kernel_initializer=init_mode, activation='relu'))\n",
    "    model_dnn.add(Dropout(dropout_rate))\n",
    "    # model.add(Dense(1, activation='relu'))\n",
    "    # model.add(Flatten())\n",
    "    model_dnn.add(Dense(2, activation='softmax'))\n",
    "    # adam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "    # model.compile(optimizer=adam,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    optimizer = keras.optimizers.Adam(lr=learn_rate)\n",
    "    model_dnn.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # model_dnn.fit(X_train, y_train, epochs=100, batch_size=64)\n",
    "    return model_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "224272/638730 [=========>....................] - ETA: 2:11 - loss: 0.2796 - acc: 0.8575"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b5ee5e8afcb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_dnn.fit(X_train, Y_train, epochs=2000, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of our deep learning network\n",
    "# from ann_visualizer.visualize import ann_viz;\n",
    "# ann_viz(model_dnn, title=\"model visualization\", view=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212911/212911 [==============================] - 12s 55us/step\n",
      "Loss = 0.227156401941\n",
      "Test Accuracy = 0.886412632508\n"
     ]
    }
   ],
   "source": [
    "# Print Test Accuracy: \n",
    "preds = model_dnn.evaluate(x = X_test, y = Y_test)\n",
    "### END CODE HERE ###\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dnn.predict(X_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638730/638730 [==============================] - 30s 47us/step\n",
      "Loss = 0.222039635102\n",
      "Train Accuracy = 0.890124152615\n"
     ]
    }
   ],
   "source": [
    "preds = model_dnn.evaluate(x = x_train, y = y_train)\n",
    "### END CODE HERE ###\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Train Accuracy = \" + str(preds[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
